{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Ch06_SparkSQL.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "quMxOZRhIjY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "rootpath = '/class/'\n",
        "datapath = f'{rootpath}datasets/'\n",
        "sys.path.append(rootpath)\n",
        "from pyspark_helpers import *\n",
        "sc, spark, conf = initspark()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5xXijGGIjY_",
        "colab_type": "text"
      },
      "source": [
        "### You can query an existing Hive table and bring it into a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDrEPFJEIjZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regions = spark.sql('select * from regions')\n",
        "Eregions = spark.read.table('regions')\n",
        "\n",
        "\n",
        "# r = spark.read.csv('hdfs://localhost:9000/user/hive/warehouse/regions', schema='regionid:int, regionname:string').where('regionid<=2')\n",
        "# r.show()\n",
        "display(regions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtDXdWU3IjZE",
        "colab_type": "text"
      },
      "source": [
        "### Read in a file to a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zam3fX90IjZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "territories = spark.read.csv(f'{datapath}/northwind/CSVHeaders/territories', header=True)\n",
        "territories.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItVQo0iIIjZI",
        "colab_type": "text"
      },
      "source": [
        "### Use createOrReplaceTempView to create a virtual table in the Hive catalog and then it can be queried using SQL as if it were a hive table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEANxCYAIjZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "territories.createOrReplaceTempView('territories')\n",
        "t1 =spark.sql('select * from territories where regionid = 1').orderBy('TerritoryName')\n",
        "t1.show()\n",
        "print(t1.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TitXo21BIjZO",
        "colab_type": "text"
      },
      "source": [
        "### Spark DataFrames can be saved to a Hive table using either the saveAsTable method or writing a SQL query that uses CREATE TABLE AS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMA5FJAIjZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! hadoop fs -rm -r /user/hive/warehouse/territories2\n",
        "! hadoop fs -rm -r /user/hive/warehouse/territories3\n",
        "! hadoop fs -rm -r /user/hive/warehouse/territoryregion\n",
        "spark.sql('drop table if exists territories3')\n",
        "\n",
        "spark.sql('CREATE TABLE territories3 STORED AS ORC AS SELECT * FROM territories ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ur1FQiIjZS",
        "colab_type": "text"
      },
      "source": [
        "### Queries use standard HQL to mix Hive tables and virtual tables. Both are read into a Spark DataFrame and the processing happens at the Spark level not at the Hive level. HQL is just used to parse the logic into the corresponding Spark methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoiAtWIEIjZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql = \"\"\"\n",
        "select r.regionid, r.regionname, t.territoryid, t.territoryname \n",
        "from regions as r \n",
        "join territories as t on r.regionid = t.regionid \n",
        "order by r.regionid, t.territoryid\n",
        "\"\"\"\n",
        "rt = spark.sql(sql)\n",
        "rt.show(10)\n",
        "\n",
        "tr = regions.join(territories, regions.regionid == territories.RegionID). \\\n",
        "     select('regions.regionid', 'regionname', 'TerritoryID', 'TerritoryName'). \\\n",
        "     orderBy('regionid', 'territoryid')\n",
        "tr.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGBGXQQaIjZX",
        "colab_type": "text"
      },
      "source": [
        "### *LAB*: Read the northwind JSON products and make it into a TempView and do the same with the CSVHeaders version of categories. Then join the two using SparkSQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usZ6vjKjIjZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################################################categories = spark.read.csv(f'{datapath}/northwind/CSVHeaders/categories', header=True, inferSchema = True)\n",
        "##############################################################################################print(categories)\n",
        "##############################################################################################display(categories)\n",
        "##############################################################################################categories.createOrReplaceTempView('categories') \n",
        "\n",
        "##############################################################################################products = spark.read.json(f'{datapath}/northwind/JSON/products')\n",
        "##############################################################################################print(products)\n",
        "##############################################################################################display(products)\n",
        "##############################################################################################products.createOrReplaceTempView('products') \n",
        "\n",
        "##############################################################################################sql = '''\n",
        "##############################################################################################select c.categoryid, c.categoryname, p.productid, p.productname, p.unitprice\n",
        "##############################################################################################from products as p\n",
        "##############################################################################################join categories as c on p.categoryid = c.categoryid\n",
        "##############################################################################################order by c.categoryid, p.productid\n",
        "##############################################################################################'''\n",
        "##############################################################################################display(spark.sql(sql))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hez2VJhfIjZc",
        "colab_type": "text"
      },
      "source": [
        "### Install the MySQL Python connector. This has nothing to do with Spark but if you want to run SQL queries directly, it is helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vehT7f5IjZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install mysql-connector-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk7SyGVNIjZh",
        "colab_type": "text"
      },
      "source": [
        "### Let's make sure we have a database for northwind and no regions table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkH6ljWqIjZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mysql.connector\n",
        "try:\n",
        "    cn = mysql.connector.connect(host='localhost', user='test', password='password')\n",
        "    cursor = cn.cursor()\n",
        "    cursor.execute('create database if not exists northwind')\n",
        "    cn.close()\n",
        "\n",
        "    cn = mysql.connector.connect(host='localhost', user='test', password='password', database='northwind')\n",
        "    cursor = cn.cursor()    \n",
        "    cursor.execute('drop table if exists regions')\n",
        "    cn.close()\n",
        "except:\n",
        "    print('something went wrong')\n",
        "else:\n",
        "    print('success')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTfWpmSUIjZl",
        "colab_type": "text"
      },
      "source": [
        "### Write a DataFrame to a SQL database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL1Vpzr7IjZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regions.write.format(\"jdbc\").mode('overwrite').options(url=\"jdbc:mysql://localhost/northwind\", driver='com.mysql.jdbc.Driver', dbtable='regions', user='test', password = \"password\", mode = \"append\", useSSL = \"false\").save()\n",
        "products.write.format(\"jdbc\").mode('overwrite').options(url=\"jdbc:mysql://localhost/northwind\", driver='com.mysql.jdbc.Driver', dbtable='products', user='test', password = \"password\", mode = \"append\", useSSL = \"false\").save()\n",
        "\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQjCqfctIjZq",
        "colab_type": "text"
      },
      "source": [
        "### Read a SQL table into a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sQIoWiIIjZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regions2 = spark.read.format(\"jdbc\"). \\\n",
        "           options(url=\"jdbc:mysql://localhost/northwind\", \n",
        "                   driver=\"com.mysql.jdbc.Driver\", \n",
        "                   dbtable= \"regions\", user=\"test\", password=\"password\").load()\n",
        "regions2.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jpfSdIVIjZv",
        "colab_type": "text"
      },
      "source": [
        "### Use the query option to do some of the initial work on the SQL side, then bring that into a dataframe and continue doing more processing with SparkSQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3fL_azeIjZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql = \"select productid, productname, unitprice, unitsinstock from products where categoryid = 2\"\n",
        "products2 = spark.read.format(\"jdbc\"). \\\n",
        "            options(url=\"jdbc:mysql://localhost/northwind\", \n",
        "                    driver=\"com.mysql.jdbc.Driver\", \n",
        "                    query=sql, user=\"test\", password=\"password\").load()\n",
        "products2.show()\n",
        "\n",
        "products2.createOrReplaceTempView('products2')\n",
        "display(spark.sql('select *, unitprice * unitsinstock as value from products2'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQg5Ee7HIjZz",
        "colab_type": "text"
      },
      "source": [
        "### Creating the regions2 DataFrame does not execute anything yet, but by making the DataFrame into a Temp View then running a Spark SQL query, it tells Spark to read the SQL data into a DataFrame and then use the cluster to do the processing, not the SQL source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MmNpUtiIjZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regions2.createOrReplaceTempView('regions2')\n",
        "spark.sql('select * from regions2 where regionid < 3').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSW1ZJkmIjZ3",
        "colab_type": "text"
      },
      "source": [
        "### Alternate ways to code a query using SQL and methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpEy8wHNIjZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = spark.sql('select count(*) from regions').collect()\n",
        "# print(x[0][0])\n",
        "# spark.sql('select * from regions').count()\n",
        "\n",
        "p2 = products.withColumn('value', products.unitprice * products.unitsinstock).where('value > 500')\n",
        "display(p2)\n",
        "\n",
        "sql = \"\"\"\n",
        "select *\n",
        "from (select *, unitprice * quantity as value) as t\n",
        "where value > 500\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3XkrnCoIjZ-",
        "colab_type": "text"
      },
      "source": [
        "### Using SQL you can use familiar syntax instead of withColumn or withCoumnRenamed methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhgJAwWAIjZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = spark.sql('select TerritoryID as TerrID, UPPER(TerritoryName) as TerritoryName, RegionID from territories')\n",
        "t1.show(5)\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "territories.withColumn('TerritoryName', expr('UPPER(TerritoryName)')).withColumnRenamed('TerritoryID', 'TerrID').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDbafgnlIjaE",
        "colab_type": "text"
      },
      "source": [
        "### Sometimes there is a function in Python that doesn't exist in SQL and it would be helpful to use, so you could make a udf and use withColumn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TavdOcvIjaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import expr, udf\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "t2 = spark.sql('select * from territories')\n",
        "t2.printSchema()\n",
        "#t2.show()\n",
        "t2 = t2.withColumn('upperName', expr('UPPER(TerritoryName)'))\n",
        "t2.show(5)\n",
        "\n",
        "t2 = t2.withColumn('titleName', udf(lambda x : x.title(), StringType())(t2.upperName))\n",
        "t2.show(5)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC6AK-BuIjaO",
        "colab_type": "text"
      },
      "source": [
        "### To make it easier though, you could make the Python function into a udf that SQL can understand similar to how you can make a DataFrame seem like a virtual table with createOrReplaceTempView."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxM6LDVDIjaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverseString(x):\n",
        "    return x[::-1]\n",
        "\n",
        "spark.udf.register('reverse', reverseString, StringType())\n",
        "\n",
        "spark.sql('select *, reverse(TerritoryName) as Reversed from Territories').orderBy('Reversed').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzmpC16ZIjaV",
        "colab_type": "text"
      },
      "source": [
        "### HQL has collect_set and collect_list functions to aggregate items into a list instead of summing them up. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M-SyfAkIjaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import collect_list\n",
        "territories.groupBy(territories.RegionID).agg(collect_list(territories.TerritoryName)).show()\n",
        "\n",
        "tr1 = spark.sql(\"SELECT RegionID, collect_list(TerritoryName) AS TerritoryList FROM Territories GROUP BY RegionID\")\n",
        "display(tr1)\n",
        "tr1.printSchema()\n",
        "print(tr1.take(1))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onkeXx4EIjaZ",
        "colab_type": "text"
      },
      "source": [
        "### Instead of a simple datatype you could also collect complex structured objects using the HQL NAMED_STRUCT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw53GGOtIjab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sql = \"\"\"SELECT r.RegionID, r.RegionName\n",
        ", COLLECT_SET(NAMED_STRUCT(\"TerritoryID\", t.TerritoryID, \"TerritoryName\", t.TerritoryName)) AS TerritoryList\n",
        "FROM Regions AS r\n",
        "JOIN Territories AS t ON r.RegionID = t.RegionID\n",
        "GROUP BY r.RegionID, r.RegionName\n",
        "ORDER BY r.RegionID\"\"\"\n",
        "\n",
        "tr2 = spark.sql(sql)\n",
        "tr2.printSchema()\n",
        "print(tr2)\n",
        "tr2.show()\n",
        "print(tr2.take(2))\n",
        "tr2.write.json('TerritoryRegion.json')\n",
        "spark.sql('create table TerritoryRegion as ' + sql)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcWXwMEnIjae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr2 = spark.read.json('TerritoryRegion.json')\n",
        "display(tr2)\n",
        "tr2.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSVtJ5h3Ijai",
        "colab_type": "text"
      },
      "source": [
        "### If you have data that is already collected into a complex datatype and want to flatten it, you could use HQL EXPLODE function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RTAGmYlIjaj",
        "colab_type": "text"
      },
      "source": [
        "### You could use the Spark explode method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AESEFGdIjaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import explode\n",
        "tr1.select('RegionID', explode('TerritoryList')).withColumnRenamed('col','TerritoryName').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKrwB2h5Ijam",
        "colab_type": "text"
      },
      "source": [
        "### Or if the DataFrame is turned into a Temp View, you could use the HQL query to do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn4VUdAfIjan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr1.createOrReplaceTempView('RegionTerritories')\n",
        "sql = \"\"\"SELECT RegionID, TerritoryName\n",
        "FROM RegionTerritories\n",
        "LATERAL VIEW EXPLODE(TerritoryList) EXPLODED_TABLE AS TerritoryName\n",
        "ORDER BY RegionID, TerritoryName\n",
        "\"\"\"\n",
        "spark.sql(sql).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW1j8yZuIjaq",
        "colab_type": "text"
      },
      "source": [
        "### Or you could select specific elements from a collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM6IeLyPIjar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr2.createOrReplaceTempView('RegionTerritories')\n",
        "spark.sql(\"select RegionId, RegionName, TerritoryList[0] as First, TerritoryList[size(TerritoryList) - 1] as Last, size(TerritoryList) as TerritoryCount from RegionTerritories\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCUdS92SIjax",
        "colab_type": "text"
      },
      "source": [
        "### If the array is of structs note the syntax of fetching the elements from the struct uses the . like an object property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNFatmfoIjay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql = \"\"\"SELECT RegionID, RegionName, Territory.TerritoryID AS TerritoryID, Territory.TerritoryName AS TerritoryName\n",
        "FROM RegionTerritories\n",
        "LATERAL VIEW EXPLODE(TerritoryList) EXPLODED_TABLE AS Territory\n",
        "\"\"\"\n",
        "spark.sql(sql).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWIU9ib0Ija1",
        "colab_type": "text"
      },
      "source": [
        "### To read from Cassandra:\n",
        "Requires PYSPARK_SUBMIT_ARGS='--packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bux6_zqEIja2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "people = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"student\", keyspace=\"classroom\").load()\n",
        "display(people)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_-aH9guIja5",
        "colab_type": "text"
      },
      "source": [
        "### To read from Mongo:\n",
        "Requires PYSPARK_SUBMIT_ARGS\"--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.1\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKG38_39Ija7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/classroom.people\").load()\n",
        "display(df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}