{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roitraining/SparkforDataEngineers/blob/Development/Ch07_ClusterAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6KkEjHKpxwC"
   },
   "source": [
    "### Initialize the spark environment and load the helper functions we have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jva8hK90pxwF"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/class/'\n",
    "datapath = f'{rootpath}datasets/'\n",
    "sys.path.append(rootpath)\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbzStKdFpxwM"
   },
   "source": [
    "### Read in a simple dataset of latitudes and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zM_IDpZzpxwN"
   },
   "outputs": [],
   "source": [
    "filename = 'superchargers.csv'\n",
    "df = spark.read.csv(f'{datapath}{filename}', header = True, inferSchema = True)\n",
    "display(df)\n",
    "\n",
    "# Save a pointer to the raw data\n",
    "dfRawFile = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3d12RxW8pxwR"
   },
   "source": [
    "### Visualize this dataset using pandas. Normally you don't do this in spark but it is helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jf3-joDrpxwS"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "p = df.toPandas()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(p.loc[:,'lng'],p.loc[:,'lat'],'o')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YNtS0k3bpxwV"
   },
   "source": [
    "### Turn the features into a big vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juAe-hNYpxwW"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"lng\", \"lat\"], outputCol=\"features\")\n",
    "dfML = vecAssembler.transform(df)  \n",
    "display(dfML)\n",
    "dfML.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiQS7aWcpxwf"
   },
   "source": [
    "### Visualize the cluster results graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7PxeQFNpxwh"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CLUSTERS = 3\n",
    "\n",
    "#kmeans = KMeans(k=2, seed = 1)\n",
    "\n",
    "kmeans = KMeans().setK(CLUSTERS).setSeed(2)\n",
    "\n",
    "# kmeans = KMeans()\n",
    "# kmeans.setK(CLUSTERS)\n",
    "# kmeans.setSeed(1)\n",
    "\n",
    "model = kmeans.fit(dfML.select('features')) # pandas dfML[['features']]\n",
    "centroids = model.clusterCenters()\n",
    "print(centroids)\n",
    "print(type(centroids[0]))\n",
    "\n",
    "predictions = model.transform(dfML) # pandas .predict\n",
    "display(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for i in range(CLUSTERS):    \n",
    "    #p = predictions.where(f'prediction = {i}').toPandas()[['lat','lng']]\n",
    "    p = predictions.select('lng', 'lat').where(f'prediction = {i}').toPandas()\n",
    "    plt.plot(p.loc[:,'lng'],p.loc[:,'lat'],'o')\n",
    "    plt.plot(centroids[i][0], \n",
    "           centroids[i][1],'kx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0KDNJ3GMpxwb"
   },
   "source": [
    "### Load the KMeans class and train the model. Evaluate how good it performs for several different cluster counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqbL9hrJpxwc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "def evaluateCluster(model, df):\n",
    "    wssse = model.computeCost(dfML.select('features'))\n",
    "    print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "    evaluator = ClusteringEvaluator()\n",
    "\n",
    "    predictions = model.transform(df)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "    # Shows the result.\n",
    "    centers = model.clusterCenters()\n",
    "    print(\"Cluster Centers: \")\n",
    "    for center in centers:\n",
    "        print(center)\n",
    "\n",
    "for k in range(2, 5): # mean 2, 3, 4 python up to but not including 5\n",
    "    print ('Number of clusters', k)\n",
    "    kmeans = KMeans().setK(k).setSeed(1)\n",
    "    model = kmeans.fit(dfML.select('features'))\n",
    "    evaluateCluster(model, dfML.select('features'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhLUXEbspxwk"
   },
   "source": [
    "### Use and elbow chart to help visualize what is the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pr1gAeuKpxwl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_elbow(df, cluster_cnt = 6):\n",
    "    import numpy as np\n",
    "    CLUSTERS = range(2, cluster_cnt)\n",
    "    scores = [KMeans().setK(c).setSeed(1).fit(df).computeCost(df)\n",
    "              for c in CLUSTERS]\n",
    "    print(scores)\n",
    "    plt.plot(CLUSTERS, scores)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Elbow Curve')\n",
    "    plt.xticks(np.arange(2, cluster_cnt))\n",
    "\n",
    "plot_elbow(dfML.select('features'), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Ch07_ClusterAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
