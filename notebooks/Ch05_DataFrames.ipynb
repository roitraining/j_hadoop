{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch05_DataFrames.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roitraining/jpmc_hadoop/blob/master/notebooks/Ch05_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-JdIfR70Jxu2"
      },
      "source": [
        "### Set up the Spark environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "os_Mz8CUJxu5",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "rootpath = '/class/'\n",
        "datapath = f'{rootpath}datasets/'\n",
        "sys.path.append(rootpath)\n",
        "from pyspark_helpers import *\n",
        "sc, spark, conf = initspark()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6vl7-cmUJxu-"
      },
      "source": [
        "### Turn a simple RDD into a DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sou-X5kCJxu_",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "x = sc.parallelize([(1,'alpha'),(2,'beta')])\n",
        "x0 = spark.createDataFrame(x)\n",
        "x0.show()\n",
        "print(x0.collect())\n",
        "list(map(tuple, x0.collect()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E_1Cg_xuJxvE"
      },
      "source": [
        "### Give the DataFrame meaningful column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "udSoOBO5JxvF",
        "colab": {}
      },
      "source": [
        "x1 = spark.createDataFrame(x, schema=['ID','Name'])\n",
        "#x1 = spark.createDataFrame(x, schema='ID, Name') # Does not work\n",
        "x1.show()\n",
        "print(x1)\n",
        "print(x1.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uiGdbXBVJxvJ"
      },
      "source": [
        "### Give a DataFrame a schema with column names and data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "en8OdGGlJxvK",
        "colab": {}
      },
      "source": [
        "x2 = spark.createDataFrame(x, schema='ID:int, Name:string')\n",
        "x2.show()\n",
        "print(x2)\n",
        "\n",
        "x3 = x2.rdd.map(lambda x : (x.ID * 10, x.Name.upper()))\n",
        "\n",
        "x4 = spark.createDataFrame(x3)\n",
        "x4.show()\n",
        "\n",
        "x5 = x3.toDF(schema='ID:int, Name:string')\n",
        "x5.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkpkmdwHeXb",
        "colab_type": "text"
      },
      "source": [
        "### Could also use a structured type object to specify schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8BVb2RsHeXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "schema = StructType([\n",
        "    StructField('ID', IntegerType()), \n",
        "    StructField('Name', StringType())\n",
        "])\n",
        "x6 = spark.createDataFrame(x, schema=schema)\n",
        "display(x6)\n",
        "\n",
        "x7 = x.toDF(schema=schema)\n",
        "display(x7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VlhoOJwZJxvO"
      },
      "source": [
        "### Load a text file into a RDD and clean it up as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YxFb9sk4JxvQ",
        "colab": {}
      },
      "source": [
        "rddRegions = sc.textFile('hdfs://localhost:9000/regions')\n",
        "rddRegions = rddRegions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
        "print(rddRegions.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mk7f9YAWJxvc"
      },
      "source": [
        "### Turn the RDD into a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y2iXnrkJJxvu",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "dfRegions = rddRegions.toDF('RegionID: int, RegionName:string')\n",
        "dfRegions.show()\n",
        "print(dfRegions)\n",
        "dfRegions.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxfPDUxzHeXq",
        "colab_type": "text"
      },
      "source": [
        "### Better yet, just use the built-in CSV reader to read a file directly into a DataFrame and skip RDDs altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH6zeTKpHeXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "territoriesFile = f'{datapath}/northwind/CSV/territories'\n",
        "df = spark.read.csv(territoriesFile, header=True, inferSchema=True)\n",
        "# this doesn't work though\n",
        "# df = spark.read.csv(filename, schema='TerritoryID:int, TerritoryName:string, RegionID:int')\n",
        "display(df)\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYfQ-bujHeXu",
        "colab_type": "text"
      },
      "source": [
        "### To read a CSV file with a specific schema you must use the StructType, you can't just pass in a string with column names and types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnNGLpNrHeXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "schTerritories = StructType([\n",
        "    StructField('TerritoryID', IntegerType()), \n",
        "    StructField('TerritoryName', StringType()), \n",
        "    StructField('RegionID', IntegerType())\n",
        "])\n",
        "dfTerritories = spark.read.csv(territoriesFile, schema = schTerritories, header=False)\n",
        "dfTerritories.printSchema()\n",
        "display(dfTerritories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rdAF7yVmJxvy"
      },
      "source": [
        "### ***LAB:*** Read regions from the the HDFS folder using read.csv and territories from the /home/student/ROI/Spark/datasets/northwind/CSVHeaders folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIbiLKz0Jxvz",
        "colab": {}
      },
      "source": [
        "shippersFile= f'{datapath}/northwind/CSVHeaders/shippers'\n",
        "##############################################################################################schShippers = StructType([\n",
        "##############################################################################################    StructField('ShipperID', IntegerType()), \n",
        "##############################################################################################    StructField('CompanyName', StringType()), \n",
        "##############################################################################################    StructField('Phone', StringType())\n",
        "##############################################################################################])\n",
        "##############################################################################################dfShippers = spark.read.csv(shippersFile, schema = schShippers, header=True)\n",
        "##############################################################################################dfShippers.printSchema()\n",
        "##############################################################################################display(dfShippers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CqpW0HPKJxv2"
      },
      "source": [
        "### Convert a DataFrame into a JSON string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I64RwvUdJxv3",
        "colab": {}
      },
      "source": [
        "print (dfShippers.toJSON().take(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h1hZdPH5Jxxm"
      },
      "source": [
        "### Read a JSON file into a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jYzveIUNJxxn",
        "colab": {}
      },
      "source": [
        "dfProducts = spark.read.json(f'{datapath}/northwind/JSON/products')\n",
        "dfProducts.printSchema()\n",
        "display(dfProducts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P2qg8kVHeYA",
        "colab_type": "text"
      },
      "source": [
        "### Here is an ORC example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8N_5lryHeYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfOrderDetails = spark.read.orc(f'{datapath}/northwind/ORC/orderdetails')\n",
        "dfOrderDetails.printSchema()\n",
        "display(dfOrderDetails)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aImBgjSCHeYF",
        "colab_type": "text"
      },
      "source": [
        "### Here is a Parquet example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bAXEK66HeYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfOrders = spark.read.parquet(f'{datapath}/northwind/PARQUET/orders')\n",
        "dfOrders.printSchema()\n",
        "display(dfOrders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s2PVAJGSJxv-"
      },
      "source": [
        "### Choose particular columns from a DataFrame and create a calculated column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "shp65QEQJxv_",
        "colab": {}
      },
      "source": [
        "# Python style\n",
        "df1 = dfProducts.select(dfProducts.unitprice, dfProducts.unitsinstock).withColumn('value', dfProducts.unitprice * dfProducts.unitsinstock)\n",
        "display(df1)\n",
        "\n",
        "# SQL style\n",
        "from pyspark.sql.functions import expr\n",
        "df1 = dfProducts.select('unitprice', 'unitsinstock').withColumn('value', expr('unitprice * unitsinstock'))\n",
        "display(df1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koaSvHoEHeYO",
        "colab_type": "text"
      },
      "source": [
        "### Try a bunch of different transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOXLR2IPJxwD",
        "colab": {}
      },
      "source": [
        "#display(dfProducts.drop('discontinued', 'reorderlevel'))\n",
        "#display(dfProducts.withColumnRenamed('discontinued', 'gone'))\n",
        "# display(dfProducts.select('supplierid').distinct())\n",
        "# print(dfProducts.select('supplierid').distinct().count())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w9NySqdqJxwH"
      },
      "source": [
        "### Sort a DataFrame. The sort and orderBy methods are different aliases for the exact same method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i9IkEP-4JxwI",
        "colab": {}
      },
      "source": [
        "#display(dfProducts.sort(dfProducts.unitprice))\n",
        "#display(dfProducts.sort(dfProducts.unitprice, ascending = False))\n",
        "#display(dfProducts.orderBy('unitprice', ascending = False))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6zM9-5MJxwS"
      },
      "source": [
        "### Create a new DataFrame with a new calculated column added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XfXZYr82JxwU",
        "colab": {}
      },
      "source": [
        "dfOrderDetails2 = dfOrderDetails.withColumn('total', dfOrderDetails.unitprice * dfOrderDetails.quantity)\n",
        "display(dfOrderDetails2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZsHhY9u9Jxwh"
      },
      "source": [
        "### The filter and where methods can both be used and have alternative ways to represent the condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-bg7uhoEJxwj",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df = dfProducts\n",
        "#display(df.filter(df.unitprice < 100))\n",
        "print(df.filter('unitprice < 100').count())\n",
        "print(df.where(df.unitprice == 10).count())\n",
        "print(df.where('unitprice = 10').count())\n",
        "\n",
        "print(df.where((df.unitprice >= 10) & (df.unitprice <=20)).count())\n",
        "print(df.where('unitprice BETWEEN 10 and 20').count())\n",
        "# print(df.where(df.Amount < 4000).count())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k5E4afFTJxwp"
      },
      "source": [
        "### ***LAB:*** Using dfOrderDetails\n",
        "\n",
        "\n",
        "*   Find out how many orders have a line total more than 1000\n",
        "*   Find the ten largest line totals\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6kZwJv9LJxwr",
        "colab": {}
      },
      "source": [
        "#display(dfOrderDetails)\n",
        "##############################################################################################df = dfOrderDetails\n",
        "##############################################################################################df1 = df.withColumn('total', expr('unitprice * quantity'))\n",
        "##############################################################################################df2 = df1.where('total > 1000')\n",
        "##############################################################################################print (df2.count())\n",
        "\n",
        "##############################################################################################display(df1.orderBy('total', ascending = False).limit(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yRYHW5DlJxwx"
      },
      "source": [
        "### JOINs work as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wLGYJT1PJxwy",
        "colab": {}
      },
      "source": [
        "tab1 = sc.parallelize([(1, 'Alpha'), (2, 'Beta'), (3, 'Delta')]).toDF('ID:int, code:string')\n",
        "tab2 = sc.parallelize([(100, 'One', 1), (101, 'Two', 2), (102, 'Three', 1), (103, 'Four', 4)]).toDF('childID:int, name:string, parentID:int')\n",
        "display(tab1)\n",
        "display(tab2)\n",
        "display(tab1.join(tab2, tab1.ID == tab2.parentID))\n",
        "display(tab1.join(tab2, tab1.ID == tab2.parentID, 'left'))\n",
        "display(tab1.join(tab2, tab1.ID == tab2.parentID, 'right'))\n",
        "display(tab1.join(tab2, tab1.ID == tab2.parentID, 'full'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FbKSw8mFJxw1"
      },
      "source": [
        "### Examples of aggregate functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q-LPpNaJxw2",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "tab3 = sc.parallelize([(1, 10), (1, 20), (1, 30), (2, 40), (2,50)]).toDF('groupID:int, amount:int')\n",
        "display(tab3)\n",
        "tab3.groupby('groupID').max().show()\n",
        "tab3.groupby('groupID').sum().show()\n",
        "x = tab3.groupby('groupID')\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "#from pyspark.sql.functions import sum, max\n",
        "# print(dir(F))\n",
        "x.agg(F.sum('amount'), F.max('amount')).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f5J3KIQfJxw5"
      },
      "source": [
        "### Examples of reading a CSV directly into a DataFrame using different styles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJWVKyXeJxw6",
        "colab": {}
      },
      "source": [
        "filename = f'{datapath}/finance/CreditCard.csv'\n",
        "dfCreditCard = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = True)\n",
        "dfCreditCard.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GmIrAg1KJxxA",
        "colab": {}
      },
      "source": [
        "df4 = spark.read.format('csv').option('header','true').option('inferSchema','true').load(filename)\n",
        "df4.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JQVzZUyVJxxE",
        "colab": {}
      },
      "source": [
        "df4 = spark.read.csv(filename, header = True, inferSchema = True)\n",
        "df4.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TlpJkJKkJxxI",
        "colab": {}
      },
      "source": [
        "display(df4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2pxxBU4AJxxS"
      },
      "source": [
        "### Another way of changing column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RUqQyieJxxU",
        "colab": {}
      },
      "source": [
        "cols = df4.columns\n",
        "cols[0] = 'CityCountry'\n",
        "df4 = df4.toDF(*cols)\n",
        "df4.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dDGFU2bwJxxN"
      },
      "source": [
        "### ***LAB:*** Read the Products file from the JSON folder and categories from the CSVHeaders folder, then join them displaying just the product and category IDs and names, and sort by categoryID then productID. \n",
        "\n",
        "\n",
        "**Hint:** Drop the ambiguous column after the join.\n",
        "Be aware of case sensitivity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTxpSZd7JxxP",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "##############################################################################################dfCategories = spark.read.csv(f'{datapath}/northwind/CSVHeaders/categories', inferSchema = True, header = True)\n",
        "##############################################################################################display(dfCategories)\n",
        "##############################################################################################dfProdCat = dfProducts.join(dfCategories, dfProducts.categoryid == dfCategories.CategoryID).drop('categoryid')\n",
        "##############################################################################################display(dfProdCat)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pk-WlDwNJxxW"
      },
      "source": [
        "### Apply a custom UDF to columns to separate the City and Country and convert the Date into a date datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4dcWXI1iJxxX",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import udf, expr\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import to_date\n",
        "import datetime\n",
        "\n",
        "def city(x):\n",
        "    return x[:x.find(',')]\n",
        "\n",
        "cityUDF = udf(city, StringType())\n",
        "\n",
        "def country(x):\n",
        "    return x[x.find(',') + 1 :]\n",
        "\n",
        "@udf(StringType())\n",
        "def gender(x):\n",
        "    return 'Male' if x == 'M' else 'Female'\n",
        "\n",
        "\n",
        "df = dfCreditCard.withColumnRenamed('City', 'CityCountry')\n",
        "df5 = df.withColumn('City', cityUDF(df.CityCountry)) \\\n",
        "      .withColumn('Country', udf(country, StringType())(df.CityCountry)) \\\n",
        "      .withColumn('Date', to_date(df.Date, 'dd-MMM-yy')) \\\n",
        "      .withColumn('Gender', gender(df.Gender)) \\\n",
        "      .drop(df.CityCountry)\n",
        "display(df5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJShtBBYJxxe"
      },
      "source": [
        "### DataFrames can be written to a variety of file formats. Here we are writing it to JSON."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lds2CsoKJxxf",
        "colab": {}
      },
      "source": [
        "df5.write.mode('overwrite').json(f'{rootpath}/CreditCard.json')\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLFj3lX_HeZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
