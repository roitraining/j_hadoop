{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Zk5rxsmgUt7"
   },
   "source": [
    "## Open a terminal window and run the following commands:\n",
    "sudo bash\n",
    "start-hadoop\n",
    "cd /home/student/ROI/SparkProgram/Day3\n",
    "./fixhive.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nT7uivAfgUuA"
   },
   "source": [
    "## Let's connect to the spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = (SparkSession.builder \n",
    "          .master(\"local[1]\") \n",
    "          .enableHiveSupport()\n",
    "          .appName(\"jupyter\") \n",
    "          .getOrCreate() \n",
    "        )\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2mr3DIKgUuL"
   },
   "source": [
    "## We can query an existing Hive table and bring it into a Spark DataFrame. First let's make sure we have a simple table in the default database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE regions(regionid int, regionname string)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "LINES TERMINATED BY '\\n' \n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/regions';\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/class/datasets/northwind/CSV/regions' OVERWRITE INTO TABLE regions;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[regionid: int, regionname: string]\n",
      "+--------+----------+\n",
      "|regionid|regionname|\n",
      "+--------+----------+\n",
      "|       1|   Eastern|\n",
      "|       2|   Western|\n",
      "|       3|  Northern|\n",
      "|       4|  Southern|\n",
      "+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "regions = spark.read.table('regions')\n",
    "print(regions)\n",
    "regions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or prefix the database name to use tables in northwind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[territoryid: string, territoryname: string, regionid: int]\n",
      "+-----------+-------------+--------+\n",
      "|territoryid|territoryname|regionid|\n",
      "+-----------+-------------+--------+\n",
      "|      01581|     Westboro|       1|\n",
      "|      01730|      Bedford|       1|\n",
      "|      01833|    Georgetow|       1|\n",
      "|      02116|       Boston|       1|\n",
      "|      02139|    Cambridge|       1|\n",
      "|      02184|    Braintree|       1|\n",
      "|      02903|   Providence|       1|\n",
      "|      03049|       Hollis|       3|\n",
      "|      03801|   Portsmouth|       3|\n",
      "|      06897|       Wilton|       1|\n",
      "|      07960|   Morristown|       1|\n",
      "|      08837|       Edison|       1|\n",
      "|      10019|     New York|       1|\n",
      "|      10038|     New York|       1|\n",
      "|      11747|     Mellvile|       1|\n",
      "|      14450|     Fairport|       1|\n",
      "|      19428| Philadelphia|       3|\n",
      "|      19713|       Neward|       1|\n",
      "|      20852|    Rockville|       1|\n",
      "|      27403|   Greensboro|       1|\n",
      "+-----------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "territories = spark.read.table('northwind.territories')\n",
    "print(territories)\n",
    "territories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a common helper function to make the output prettier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(df, limit = 10):\n",
    "    from IPython.display import display    \n",
    "    display(df.limit(limit).toPandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>territoryid</th>\n",
       "      <th>territoryname</th>\n",
       "      <th>regionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01581</td>\n",
       "      <td>Westboro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01730</td>\n",
       "      <td>Bedford</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01833</td>\n",
       "      <td>Georgetow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02116</td>\n",
       "      <td>Boston</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02139</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02184</td>\n",
       "      <td>Braintree</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>02903</td>\n",
       "      <td>Providence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>03049</td>\n",
       "      <td>Hollis</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>03801</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>06897</td>\n",
       "      <td>Wilton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  territoryid territoryname  regionid\n",
       "0       01581      Westboro         1\n",
       "1       01730       Bedford         1\n",
       "2       01833     Georgetow         1\n",
       "3       02116        Boston         1\n",
       "4       02139     Cambridge         1\n",
       "5       02184     Braintree         1\n",
       "6       02903    Providence         1\n",
       "7       03049        Hollis         3\n",
       "8       03801    Portsmouth         3\n",
       "9       06897        Wilton         1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(territories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can install this magic add in which lets us call SparkSQL directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install sparksql-magic\n",
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">regionid</td><td style=\"font-weight: bold\">regionname</td></tr><tr><td>null</td><td>RegionName</td></tr><tr><td>1</td><td>Eastern</td></tr><tr><td>2</td><td>Western</td></tr><tr><td>3</td><td>Northern</td></tr><tr><td>4</td><td>Southern</td></tr><tr><td>5</td><td>Europe</td></tr><tr><td>6</td><td>Asia</td></tr><tr><td>7</td><td></td></tr><tr><td>8</td><td></td></tr><tr><td>9</td><td>null</td></tr><tr><td>null</td><td>Wrong Number</td></tr><tr><td>null</td><td>null</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from northwind.regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can also capture the results to a variable and use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capture dataframe to local variable `regions1`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">regionname</td><td style=\"font-weight: bold\">regionid</td></tr><tr><td>Eastern</td><td>1</td></tr><tr><td>Western</td><td>2</td></tr><tr><td>Northern</td><td>3</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql regions1\n",
    "select regionname, regionid from regions where regionid < 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's basically the same as this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions2 = spark.sql('select regionname, regionid from regions where regionid < 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have a reference to a DataFrame we could use dot methods on it to further process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regionname</th>\n",
       "      <th>regionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eastern</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northern</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Western</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  regionname  regionid\n",
       "0    Eastern         1\n",
       "1   Northern         3\n",
       "2    Western         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sort_field = 'regionname'\n",
    "display(regions1.orderBy(sort_field))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can mix and match SQL and dot syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regionname</th>\n",
       "      <th>regionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eastern</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northern</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Western</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  regionname  regionid\n",
       "0    Eastern         1\n",
       "1   Northern         3\n",
       "2    Western         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regions3 = spark.sql('select regionname, regionid from regions where regionid < 4 order by regionname')\n",
    "regions3 = spark.sql('select regionname, regionid from regions where regionid < 4').sort('regionname')\n",
    "regions3 = spark.sql('select regionname, regionid from regions').filter('regionid < 4').sort('regionname')\n",
    "\n",
    "display(regions3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could also read file directly without the need to have them pre-defined in Hive first. This makes things more flexible and allows for reading more file types and for dynamic schema resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7uNzTBr_gUuR"
   },
   "source": [
    "## Read in a file to a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9O2HurUbgUuS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[TerritoryID: int, TerritoryName: string, RegionID: int]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>TerritoryName</th>\n",
       "      <th>RegionID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1581</td>\n",
       "      <td>Westboro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1730</td>\n",
       "      <td>Bedford</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1833</td>\n",
       "      <td>Georgetow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2116</td>\n",
       "      <td>Boston</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2139</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2184</td>\n",
       "      <td>Braintree</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2903</td>\n",
       "      <td>Providence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3049</td>\n",
       "      <td>Hollis</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3801</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6897</td>\n",
       "      <td>Wilton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TerritoryID TerritoryName  RegionID\n",
       "0         1581      Westboro         1\n",
       "1         1730       Bedford         1\n",
       "2         1833     Georgetow         1\n",
       "3         2116        Boston         1\n",
       "4         2139     Cambridge         1\n",
       "5         2184     Braintree         1\n",
       "6         2903    Providence         1\n",
       "7         3049        Hollis         3\n",
       "8         3801    Portsmouth         3\n",
       "9         6897        Wilton         1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "territories = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/territories'\n",
    "                             , inferSchema = True, header=True)\n",
    "print(territories)\n",
    "display(territories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But if we want we can define a specific schema and use it instead of dynamic resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pe14VoNmgUuM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regionid</th>\n",
       "      <th>regionname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Eastern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Northern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   regionid regionname\n",
       "0         1    Eastern\n",
       "1         2    Western\n",
       "2         3   Northern\n",
       "3         4   Southern"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "region_schema = StructType([\n",
    "    StructField('regionid', IntegerType()), \n",
    "    StructField('regionname', StringType())\n",
    "])\n",
    "\n",
    "regions4 = spark.read.csv('hdfs://localhost:9000/regions', schema = region_schema, header = False, sep = ',')\n",
    "\n",
    "display(regions4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Really the SQL is being converted to something like the following Spark commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capture dataframe to local variable `regions5`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">regionname</td><td style=\"font-weight: bold\">regionid</td></tr><tr><td>Eastern</td><td>1</td></tr><tr><td>Western</td><td>2</td></tr><tr><td>Northern</td><td>3</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql regions5\n",
    "SELECT regionname, regionid FROM regions WHERE regionid < 4 ORDER BY regionid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regionname</th>\n",
       "      <th>regionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eastern</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Western</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Northern</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  regionname  regionid\n",
       "0    Eastern         1\n",
       "1    Western         2\n",
       "2   Northern         3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regionschema = StructType([\n",
    "    StructField('regionid', IntegerType()), \n",
    "    StructField('regionname', StringType())\n",
    "])\n",
    "\n",
    "regions6 = (spark.read.csv('hdfs://localhost:9000/regions', header=False\n",
    "                         , schema = regionschema, sep = ',')\n",
    "                .select('regionname', 'regionid')\n",
    "                .where('regionid < 4')\n",
    "                .sort('regionid'))\n",
    "display(regions6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55rxI7rhgUuW"
   },
   "source": [
    "## Use createOrReplaceTempView to create a virtual table in the Hive catalog and then it can be queried using SQL as if it were a hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>TerritoryName</th>\n",
       "      <th>RegionID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1581</td>\n",
       "      <td>Westboro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1730</td>\n",
       "      <td>Bedford</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1833</td>\n",
       "      <td>Georgetow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2116</td>\n",
       "      <td>Boston</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2139</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2184</td>\n",
       "      <td>Braintree</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2903</td>\n",
       "      <td>Providence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3049</td>\n",
       "      <td>Hollis</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3801</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6897</td>\n",
       "      <td>Wilton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TerritoryID TerritoryName  RegionID\n",
       "0         1581      Westboro         1\n",
       "1         1730       Bedford         1\n",
       "2         1833     Georgetow         1\n",
       "3         2116        Boston         1\n",
       "4         2139     Cambridge         1\n",
       "5         2184     Braintree         1\n",
       "6         2903    Providence         1\n",
       "7         3049        Hollis         3\n",
       "8         3801    Portsmouth         3\n",
       "9         6897        Wilton         1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the territories we defined a few blocks up, not the one in the Hive catalog\n",
    "display(territories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1QjuhcTgUuX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regionid</th>\n",
       "      <th>territoryid</th>\n",
       "      <th>territoryname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1581</td>\n",
       "      <td>Westboro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1730</td>\n",
       "      <td>Bedford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1833</td>\n",
       "      <td>Georgetow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2116</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2139</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2184</td>\n",
       "      <td>Braintree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2903</td>\n",
       "      <td>Providence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>6897</td>\n",
       "      <td>Wilton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7960</td>\n",
       "      <td>Morristown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>8837</td>\n",
       "      <td>Edison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   regionid  territoryid territoryname\n",
       "0         1         1581      Westboro\n",
       "1         1         1730       Bedford\n",
       "2         1         1833     Georgetow\n",
       "3         1         2116        Boston\n",
       "4         1         2139     Cambridge\n",
       "5         1         2184     Braintree\n",
       "6         1         2903    Providence\n",
       "7         1         6897        Wilton\n",
       "8         1         7960    Morristown\n",
       "9         1         8837        Edison"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regionid</th>\n",
       "      <th>territoryid</th>\n",
       "      <th>territoryname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1581</td>\n",
       "      <td>Westboro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1730</td>\n",
       "      <td>Bedford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1833</td>\n",
       "      <td>Georgetow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2116</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2139</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2184</td>\n",
       "      <td>Braintree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2903</td>\n",
       "      <td>Providence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>6897</td>\n",
       "      <td>Wilton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7960</td>\n",
       "      <td>Morristown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>8837</td>\n",
       "      <td>Edison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   regionid  territoryid territoryname\n",
       "0         1         1581      Westboro\n",
       "1         1         1730       Bedford\n",
       "2         1         1833     Georgetow\n",
       "3         1         2116        Boston\n",
       "4         1         2139     Cambridge\n",
       "5         1         2184     Braintree\n",
       "6         1         2903    Providence\n",
       "7         1         6897        Wilton\n",
       "8         1         7960    Morristown\n",
       "9         1         8837        Edison"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "territories.createOrReplaceTempView('territories')\n",
    "t1 = spark.sql('SELECT regionid, territoryid, territoryname FROM territories WHERE regionid = 1')\n",
    "display(t1)\n",
    "print(t1.count())\n",
    "\n",
    "display(territories.select('regionid', 'territoryid', 'territoryname').filter('regionid=1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the regions table from Hive and the territories DataFrame we locally defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">regionid</td><td style=\"font-weight: bold\">regionname</td><td style=\"font-weight: bold\">territoryid</td><td style=\"font-weight: bold\">territoryname</td></tr><tr><td>1</td><td>Eastern</td><td>1581</td><td>Westboro</td></tr><tr><td>1</td><td>Eastern</td><td>1730</td><td>Bedford</td></tr><tr><td>1</td><td>Eastern</td><td>1833</td><td>Georgetow</td></tr><tr><td>1</td><td>Eastern</td><td>2116</td><td>Boston</td></tr><tr><td>1</td><td>Eastern</td><td>2139</td><td>Cambridge</td></tr><tr><td>1</td><td>Eastern</td><td>2184</td><td>Braintree</td></tr><tr><td>1</td><td>Eastern</td><td>2903</td><td>Providence</td></tr><tr><td>1</td><td>Eastern</td><td>6897</td><td>Wilton</td></tr><tr><td>1</td><td>Eastern</td><td>7960</td><td>Morristown</td></tr><tr><td>1</td><td>Eastern</td><td>8837</td><td>Edison</td></tr><tr><td>1</td><td>Eastern</td><td>10019</td><td>New York</td></tr><tr><td>1</td><td>Eastern</td><td>10038</td><td>New York</td></tr><tr><td>1</td><td>Eastern</td><td>11747</td><td>Mellvile</td></tr><tr><td>1</td><td>Eastern</td><td>14450</td><td>Fairport</td></tr><tr><td>1</td><td>Eastern</td><td>19713</td><td>Neward</td></tr><tr><td>1</td><td>Eastern</td><td>20852</td><td>Rockville</td></tr><tr><td>1</td><td>Eastern</td><td>27403</td><td>Greensboro</td></tr><tr><td>1</td><td>Eastern</td><td>27511</td><td>Cary</td></tr><tr><td>1</td><td>Eastern</td><td>40222</td><td>Louisville</td></tr><tr><td>2</td><td>Western</td><td>60179</td><td>Hoffman Estates</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select r.regionid, r.regionname, t.territoryid, t.territoryname\n",
    "from regions as r\n",
    "join territories as t on r.regionid = t.regionid\n",
    "order by r.regionid, t.territoryid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AG_7asYgUub"
   },
   "source": [
    "## Spark DataFrames can be saved to a Hive table using either the saveAsTable method or writing a SQL query that uses CREATE TABLE AS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method saveAsTable in module pyspark.sql.readwriter:\n",
      "\n",
      "saveAsTable(name, format=None, mode=None, partitionBy=None, **options) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` as the specified table.\n",
      "    \n",
      "    In the case the table already exists, behavior of this function depends on the\n",
      "    save mode, specified by the `mode` function (default to throwing an exception).\n",
      "    When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
      "    the same as that of the existing table.\n",
      "    \n",
      "    * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      "    * `overwrite`: Overwrite existing data.\n",
      "    * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      "    * `ignore`: Silently ignore this operation if data already exists.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    When `mode` is `Append`, if there is an existing table, we will use the format and\n",
      "    options of the existing table. The column order in the schema of the :class:`DataFrame`\n",
      "    doesn't need to be same as that of the existing table. Unlike\n",
      "    :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n",
      "    column names to find the correct column positions.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        the table name\n",
      "    format : str, optional\n",
      "        the format used to save\n",
      "    mode : str, optional\n",
      "        one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
      "    partitionBy : str or list\n",
      "        names of partitioning columns\n",
      "    **options : dict\n",
      "        all other string options\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(territories.write.saveAsTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_t3CY5kgUud"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Can not create the managed table('`territories3`'). The associated location('hdfs://localhost:9000/user/hive/warehouse/territories3') already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13412/1345410238.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mterritories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CSV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'territories3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#territories.write.saveAsTable('territories2', mode='overwrite')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#territories.write.saveAsTable('territories3', mode='overwrite', format='csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#spark.sql(\"create table territories4 STORED AS AVRO LOCATION '/territories4' as select * from territories\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Can not create the managed table('`territories3`'). The associated location('hdfs://localhost:9000/user/hive/warehouse/territories3') already exists."
     ]
    }
   ],
   "source": [
    "territories.write.mode('overwrite').format('CSV').saveAsTable('territories3')\n",
    "#territories.write.saveAsTable('territories2', mode='overwrite')\n",
    "#territories.write.saveAsTable('territories3', mode='overwrite', format='csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table territories4 STORED AS PARQUET LOCATION '/territories4' as select * from territories\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YgAW8SdgUuh"
   },
   "source": [
    "## Queries use standard HQL to mix Hive tables and virtual tables. Both are read into a Spark DataFrame and the processing happens at the Spark level, not at the Hive level. HQL is just used to parse the logic into the corresponding Spark methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5TTOFx_gUui"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select r.regionid, r.regionname, t.territoryid, t.territoryname \n",
    "from regions as r \n",
    "join territories as t on r.regionid = t.regionid \n",
    "order by r.regionid, t.territoryid\n",
    "\"\"\"\n",
    "rt = spark.sql(query)\n",
    "rt.show(10)\n",
    "\n",
    "tr = regions.join(territories, regions.regionid == territories.RegionID). \\\n",
    "     select('regions.regionid', 'regionname', 'TerritoryID', 'TerritoryName') .\\\n",
    "    orderBy('regionid', 'territoryid')\n",
    "tr.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBAmE5RzgUul"
   },
   "source": [
    "## LAB: ## \n",
    "### Read the northwind JSON products and make it into a TempView and do the same with the CSVHeaders version of categories.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Look at Day 2 or below to copy the code to load in the DataFrames\n",
    "<br>\n",
    "Turn each DataFrame into a temporary view\n",
    "<br>\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "categories = spark.read.load('file:///class/datasets/northwind/CSVHeaders/categories', format = 'csv', sep = ',', inferSchema = True, header = True)\n",
    "\n",
    "prodSchema = StructType([\n",
    "    StructField('productid', IntegerType()), \n",
    "    StructField('productname', StringType()),\n",
    "    StructField('supplierid', IntegerType()), \n",
    "    StructField('categoryid', IntegerType()), \n",
    "    StructField('quantityperunit', StringType()), \n",
    "    StructField('unitprice', FloatType()), \n",
    "    StructField('unitsinstock', IntegerType()), \n",
    "    StructField('unitsonorder', IntegerType()), \n",
    "    StructField('reorderlevel', IntegerType()), \n",
    "    StructField('discontinued', IntegerType())\n",
    "])\n",
    "products = spark.read.json('file:///class/datasets/northwind/JSON/products', schema=prodSchema)\n",
    "\n",
    "categories.createOrReplaceTempView('categories')\n",
    "products.createOrReplaceTempView('products')```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kReWCLN1gUuq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from products as p\n",
    "join categories as c on p.categoryid = c.categoryid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIronhy5gUut"
   },
   "source": [
    "## Install the MySQL Python connector. This has nothing to do with Spark, but if you want to run SQL queries directly it is helpful.\n",
    "It's already on our machines so we don't need to run this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsdTYGVygUuu"
   },
   "outputs": [],
   "source": [
    "! pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVmFdwergUuz"
   },
   "source": [
    "## Let's make sure we have a database for northwind and no regions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bp7a-BNdgUu0"
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "try:\n",
    "    cn = mysql.connector.connect(host='localhost', user='test', password='password')\n",
    "    cursor = cn.cursor()\n",
    "    cursor.execute('create database if not exists northwind')\n",
    "    cn.close()\n",
    "\n",
    "    cn = mysql.connector.connect(host='localhost', user='test', password='password', database='northwind')\n",
    "    cursor = cn.cursor()    \n",
    "    cursor.execute('drop table if exists regions')\n",
    "    cn.close()\n",
    "except:\n",
    "    print('something went wrong')\n",
    "else:\n",
    "    print('success')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ix21bUHogUu4"
   },
   "source": [
    "## Write a DataFrame to a SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9aHMSK3gUu5"
   },
   "outputs": [],
   "source": [
    "regions.write.format(\"jdbc\").options(url=\"jdbc:mysql://localhost/northwind\", \\\n",
    "                                     driver='com.mysql.jdbc.Driver', \\\n",
    "                                     dbtable='regions', \\\n",
    "                                     user='test', password = \"password\", mode = \"append\", \\\n",
    "                                     useSSL = \"false\").save()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cg0NxXW3gUvA"
   },
   "source": [
    "## Read a SQL table into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WE1ymq3KgUvC"
   },
   "outputs": [],
   "source": [
    "regions2 = spark.read.format(\"jdbc\").options(url=\"jdbc:mysql://localhost/northwind\", \\\n",
    "                                             driver=\"com.mysql.jdbc.Driver\", \\\n",
    "                                             dbtable= \"regions\", \\\n",
    "                                             user=\"test\", password=\"password\").load()\n",
    "regions2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ftyax8bwoI0A"
   },
   "source": [
    "## If you don't wont to bring an entire SQL table into a spark DataFrame use query instead of dbtable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNCaIv8coI0B"
   },
   "outputs": [],
   "source": [
    "regions3 = spark.read.format(\"jdbc\").options(url=\"jdbc:mysql://localhost/northwind\", \\\n",
    "                                             driver=\"com.mysql.jdbc.Driver\", \\\n",
    "                                             query= \"select regionid, regionname from regions where regionid < 3\", \\\n",
    "                                             user=\"test\", password=\"password\").load()\n",
    "regions3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZfSSRw3gUvF"
   },
   "source": [
    "## Creating the regions2 DataFrame does not execute anything yet, but by making the DataFrame into a Temp View then running a Spark SQL query, it tells Spark to read the SQL data into a DataFrame and then use the cluster to do the processing, not the SQL source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqdUrSUTgUvG"
   },
   "outputs": [],
   "source": [
    "regions2.createOrReplaceTempView('regions2')\n",
    "spark.sql('select * from regions2 where regionid < 3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jp8yIQMBoI0I"
   },
   "outputs": [],
   "source": [
    "spark.read.table('regions2').where('regionid < 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xt_OCqzCgUvK"
   },
   "source": [
    "## Alternate ways to code a query using SQL and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTph5vnOgUvL"
   },
   "outputs": [],
   "source": [
    "x = spark.sql('select count(*) from northwind.regions').collect()\n",
    "print(x)\n",
    "print(x[0], x[0]['count(1)'])\n",
    "\n",
    "x = spark.sql('select count(*) as cnt from northwind.regions').collect()\n",
    "print(x)\n",
    "print(x[0], x[0].cnt)\n",
    "\n",
    "print(spark.sql('select * from northwind.regions').count())\n",
    "print(spark.read.table('northwind.regions').count())\n",
    "\n",
    "print(spark.sql('select * from northwind.regions where regionid < 4').count())\n",
    "print(spark.read.table('northwind.regions').where('regionid<4').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQf2QOj2gUvQ"
   },
   "source": [
    "## Using SQL you can use familiar syntax instead of withColumn or withColumnRenamed methods.\n",
    "Note the expr function needs to be imported when you want to use a stringified SQL function using dot syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p18BQ_pgUvR"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "t1 = spark.sql('select TerritoryID as TerrID, UPPER(TerritoryName) as TerritoryName, RegionID from northwind.territories')\n",
    "t1.show(5)\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "territories.withColumn('TerritoryName', expr('UPPER(TerritoryName)')).withColumnRenamed('TerritoryID', 'TerrID').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsi73PpHoI0f"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# This won't work though if you want to use python functions, you need to go another step\n",
    "# territories.withColumn('TerritoryName', territories.TerritoryName.upper()).show()\n",
    "\n",
    "# You need to make the python function callable by spark by wrapping it in the udf function\n",
    "# which tells spark what datatype it returns\n",
    "\n",
    "def joeyfunc(x: str) -> str:\n",
    "    return x[::-1]\n",
    "\n",
    "#territories.withColumn('TerritoryName', udf(str.title, StringType())(territories.TerritoryName)).show()\n",
    "territories.withColumn('TerritoryName', udf(joeyfunc, StringType())(territories.TerritoryName)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1_Baqf6gUvV"
   },
   "source": [
    "## If you want to use a function that is not a standard Python or SQL function, you can always create one in Python and make it callable from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DZ-p4MmoI0i"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def inventoryvalue(quantity: int, price: float) -> float:\n",
    "    return quantity * price\n",
    "\n",
    "def inventoryvalue(quantity, price):\n",
    "    return quantity * price\n",
    "\n",
    "# Turn the Python function into a Spark callable function\n",
    "invvalue = udf(inventoryvalue, FloatType())\n",
    "p = products\n",
    "p2 = p.withColumn('value', invvalue(p.unitsinstock, p.unitprice))\n",
    "display(p2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ANiYGYGoI0m"
   },
   "source": [
    "## Python decorators are an even better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQUKWKd3oI0n"
   },
   "outputs": [],
   "source": [
    "@udf(FloatType())\n",
    "def inventoryvalue(quantity, price):\n",
    "    return quantity * price\n",
    "\n",
    "p2 = p.withColumn('value', inventoryvalue(p.unitsinstock, p.unitprice))\n",
    "display(p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83gxIGtQoI0p"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def inventoryvalue(quantity, price):\n",
    "    return quantity * price\n",
    "\n",
    "# Or dynamically wrap it, but it's harder to read\n",
    "p2 = p.withColumn('value', udf(inventoryvalue, FloatType())(p.unitsinstock, p.unitprice))\n",
    "display(p2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scBBkmrFgUvd"
   },
   "source": [
    "## To make it easier though, you could make the Python function into a udf that SQL can understand similar to how you can make a DataFrame seem like a virtual table with createOrReplaceTempView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A64igGvvgUvh"
   },
   "outputs": [],
   "source": [
    "def reverseString(x):\n",
    "    return x[::-1]\n",
    "\n",
    "spark.udf.register('reverse', reverseString, StringType())\n",
    "\n",
    "spark.sql('select *, reverse(TerritoryName) as Reversed from Territories').orderBy('Reversed').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11AafIOegUvl"
   },
   "source": [
    "## HQL has collect_set and collect_list functions to aggregate items into a list instead of summing them up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRm3IkEpgUvl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "territories.groupBy(territories.RegionID).agg(collect_list(territories.TerritoryName)).show()\n",
    "\n",
    "tr1 = spark.sql(\"SELECT RegionID, collect_list(TerritoryName) AS TerritoryList FROM Territories GROUP BY RegionID\")\n",
    "tr1.show()\n",
    "tr1.printSchema()\n",
    "print(tr1.take(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-onOtyzgUvo"
   },
   "source": [
    "## Instead of a simple datatype, you could also collect complex structured objects using the HQL NAMED_STRUCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tolziu_tgUvp"
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT r.RegionID, r.RegionName\n",
    ", COLLECT_SET(NAMED_STRUCT(\"TerritoryID\", TerritoryID, \"TerritoryName\", TerritoryName)) AS TerritoryList\n",
    "FROM northwind.Regions AS r\n",
    "JOIN Territories AS t ON r.RegionID = t.RegionID\n",
    "GROUP BY r.RegionID, r.RegionName\n",
    "ORDER BY r.RegionID\n",
    "\"\"\"\n",
    "\n",
    "tr2 = spark.sql(sql)\n",
    "tr2.printSchema()\n",
    "print(tr2)\n",
    "tr2.show()\n",
    "print(tr2.take(2))\n",
    "tr2.write.json('TerritoryRegion.json')\n",
    "spark.sql('create table TerritoryRegion as ' + sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4VnZ0MKgUvs"
   },
   "source": [
    "## If you have data that is already collected into a complex datatype and want to flatten it, you could use HQL EXPLODE function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paYQteGWgUvt"
   },
   "source": [
    "## You could use the Spark explode method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rSesh3PgUvu"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "tr1.select('RegionID', explode('TerritoryList')).withColumnRenamed('col','TerritoryName').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXxOSvqAgUvx"
   },
   "source": [
    "## Or if the DataFrame is turned into a Temp View, you could use the HQL query to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnianeudgUvy"
   },
   "outputs": [],
   "source": [
    "tr1.createOrReplaceTempView('RegionTerritories')\n",
    "sql = \"\"\"\n",
    "SELECT RegionID, TerritoryName\n",
    "FROM RegionTerritories\n",
    "LATERAL VIEW EXPLODE(TerritoryList) EXPLODED_TABLE AS TerritoryName\n",
    "ORDER BY RegionID, TerritoryName\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4XqNW4xgUv2"
   },
   "source": [
    "## Or you could select specific elements from a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYvKH3VxgUv3"
   },
   "outputs": [],
   "source": [
    "tr2.createOrReplaceTempView('RegionTerritories')\n",
    "spark.sql(\"select RegionId, RegionName, TerritoryList[0] as First, TerritoryList[size(TerritoryList) - 1] as Last, size(TerritoryList) as TerritoryCount from RegionTerritories\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgXXtetSgUv6"
   },
   "source": [
    "## If the array is of structs, note the syntax of fetching the elements from the struct uses the . like an object property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4iyj0JAgUv7"
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT RegionID, RegionName, Territory.TerritoryID AS TerritoryID\n",
    ", Territory.TerritoryName AS TerritoryName\n",
    "FROM RegionTerritories\n",
    "LATERAL VIEW EXPLODE(TerritoryList) EXPLODED_TABLE AS Territory\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDIPBzWWgUwA"
   },
   "source": [
    "## HOMEWORK: ## \n",
    "**First Challenge**\n",
    "\n",
    "Create a Python function to determine if a number is odd or even and use that to select only the even numbered shippers from the TSV folder of northwind. Note the TSV file does not have headers so you will need to do something to make the DataFrame have a meaningful structure. I would suggest using Spark SQL as much as possible to rename and cast the columns which are ShipperID, CompanyName, and Phone.\n",
    "\n",
    "**Second Challenge**\n",
    "\n",
    "Take the Order_LineItems.json folder, read it into a DataFrame, and flatten it and then calculate the average price paid for a product.\n",
    "\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Take a look at the MakeOrders_LineItems.py file provided to see how the Order_LineItems.json was generated in the first place\n",
    "<br>\n",
    "Use modulus with remainder of zero to determine if something is even\n",
    "<br>\n",
    "Use udf to make a version of the function that is callable using dot syntax and udf.register to make a version callable from within a SQL string\n",
    "<br>\n",
    "Use LATERAL VIEW EXPLODE() EXPLODED_TABLE to flatten out the nested format file\n",
    "<br>\n",
    "Once flattened do a traditional aggregate to calculate the average\n",
    "<br>\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day3-SparkSQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
