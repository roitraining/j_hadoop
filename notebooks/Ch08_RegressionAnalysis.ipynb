{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch08_RegressionAnalysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roitraining/jpmc_hadoop/blob/master/notebooks/Ch08_RegressionAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ng2_--H6q0C_"
      },
      "source": [
        "### Initialize the spark environment and load the helper functions we have provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "itiXd2lDq0DD",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "rootpath = '/class/'\n",
        "datapath = f'{rootpath}datasets/'\n",
        "sys.path.append(rootpath)\n",
        "import pyspark_helpers as pyh\n",
        "from pyspark_helpers import *\n",
        "sc, spark, conf = initspark()\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib as mp\n",
        "import numpy\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from pyspark_helpers import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G73tQ_vxq0DJ"
      },
      "source": [
        "### Read in a simple dataset of Boston Housing Prices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ppQVsqVLq0DL",
        "colab": {}
      },
      "source": [
        "filename = 'boston.csv'\n",
        "df = spark.read.csv(f'{datapath}/finance/{filename}', header = True, inferSchema = True)\n",
        "display(df)\n",
        "df.printSchema()\n",
        "\n",
        "# Save a pointer to the raw data\n",
        "dfRaw = df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qONk0oPJ6WZ",
        "colab_type": "text"
      },
      "source": [
        "### Let's have a look at the Median Value, which is the target we want to predict.\n",
        "Spark does not have plotting of it's own, instead we bring back the data to the driver to plot. So we need to make sure not to bring back more than the driver can handle.\n",
        "The .toPandas method will act like collect but bring it back as a Pandas DataFrame which is easily plotted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HAgDBLRwq0Db",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "#sns.distplot(df.toPandas()['MEDV'])\n",
        "\n",
        "sns.distplot(df.select('MEDV').toPandas())\n",
        "#sns.distplot(df.toPandas()['MEDV'])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6FsyRhJ6Wf",
        "colab_type": "text"
      },
      "source": [
        "### There's some outlier data there past 48 so let's just eliminate it for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YllbjvlJ6Wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(df.where('MEDV < 48').select('MEDV').toPandas())\n",
        "print(df.columns)\n",
        "\n",
        "# If we want to filter out the outliers\n",
        "dfRaw = dfRaw.where('MEDV < 48')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNDP-YFbJ6Wr",
        "colab_type": "text"
      },
      "source": [
        "### Let's look out the result of StringIndex to understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Typ_3M2JJ6Wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sometowns = df.where(\"TOWN IN ('Nahant', 'Swampscott','Marblehead','Salem')\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Zm7_7wFq0DP",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "col = 'TOWN'\n",
        "indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
        "x1 = indexer.fit(sometowns)\n",
        "x2 = x1.transform(sometowns).select(col, col+'_Index').distinct()\n",
        "#x2.cache()\n",
        "print(x2.select('TOWN').distinct().count())\n",
        "display(x2.orderBy(col))\n",
        "display(x2.orderBy(col+'_Index'))\n",
        "#x2.unpersist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0CDtfLLJ6XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "\n",
        "encoder = OneHotEncoderEstimator(inputCols=['TOWN_Index'], outputCols=['TOWN_Vector'])\n",
        "display(encoder.fit(x2).transform(x2).orderBy(col + '_Index'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkUOcGQqJ6XN",
        "colab_type": "text"
      },
      "source": [
        "### Now try it with a convenient helper function we wrote to encode a list of multiple columns automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7_7AYzbiq0DS",
        "colab": {}
      },
      "source": [
        "# import imp\n",
        "# imp.reload(pyh)\n",
        "\n",
        "# x2 = pyh.StringIndexEncode(df, ['TOWN', 'TRACT'], return_key_dict = False)\n",
        "# display (x2)\n",
        "\n",
        "x2, x3 = pyh.StringIndexEncode(df, ['TOWN', 'TRACT'], return_key_dict = True)\n",
        "display (x2)\n",
        "print(x3)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHnSyBjUJ6XX",
        "colab_type": "text"
      },
      "source": [
        "### Now try our convenient helper function. Note that it automatically called StringIndexer first so we can work on the raw string version of the column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoL10LsKJ6Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x4 = pyh.OneHotEncode(x2, ['TOWN', 'TRACT'])\n",
        "display (x4)\n",
        "\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "vecAssembler = VectorAssembler(inputCols=[\"AGE\",\"CHAS\",\"DIS\", \"TOWN_Vector\"], outputCol=\"features\")\n",
        "dfx4 = vecAssembler.transform(x4).withColumnRenamed('MEDV','target').select('features', 'target')\n",
        "display(dfx4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgDuPjYwJ6Xf",
        "colab_type": "text"
      },
      "source": [
        "### Let's put it all together now. Identify the categorical and numeric features and target.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cDJ269zdq0De",
        "colab": {}
      },
      "source": [
        "numeric_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO']\n",
        "categorical_features = ['TOWN', 'TRACT']\n",
        "target_label = 'MEDV'\n",
        "df = dfRaw.select(categorical_features + numeric_features + [target_label])\n",
        "df.printSchema()\n",
        "\n",
        "print ('******')\n",
        "display(df.describe())\n",
        "\n",
        "print ('******')\n",
        "display(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "egQIxlV9q0Di"
      },
      "source": [
        "### Turn the dataframe into vectors.\n",
        "Use our MakeMLDataFrame helper function to automatically encode the list of categorical values, and bundle everything up into a features and target vector as needed for ML training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YvJc1VyJ6Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import imp\n",
        "# imp.reload(pyh)\n",
        "\n",
        "keydict = {}\n",
        "# Features but no target\n",
        "#dfML = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, None, target_is_categorical = False, return_key_dict = False)\n",
        "#dfML, keydict = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, None, target_is_categorical = False, return_key_dict = True)\n",
        "\n",
        "# Features and categorical target\n",
        "#dfML = pyh.MakeMLDataFrame(df, ['TOWN'], numeric_features, 'TRACT', target_is_categorical = True, return_key_dict = False)\n",
        "#dfML, keydict = pyh.MakeMLDataFrame(df, ['TOWN'], numeric_features, 'TRACT', target_is_categorical = True, return_key_dict = True)\n",
        "\n",
        "# Features and non-categorical target\n",
        "dfML = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, target_label, target_is_categorical = False, return_key_dict = False)\n",
        "#dfML, keydict = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, target_label, target_is_categorical = False, return_key_dict = True)\n",
        "\n",
        "\n",
        "print(keydict.keys())\n",
        "display(dfML)\n",
        "dfML.printSchema()\n",
        "print(keydict)\n",
        "print(dfML.take(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kaAFz43aq0Do"
      },
      "source": [
        "### Split the dataset into train and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6E281O5cq0Dp",
        "colab": {}
      },
      "source": [
        "# help(dfML.randomSplit)\n",
        "print(dfML.count())\n",
        "\n",
        "# recipe to sample 25% of the data put 20% into train, 5% into test, ignore 75%\n",
        "# train, test, _ = dfML.randomSplit([.2, .05, .75], seed = 1000)\n",
        "\n",
        "train, test = dfML.randomSplit([.7,.3], seed = 1000)\n",
        "print (f'Training set row count {train.count()}')\n",
        "print (f'Testing set row count {test.count()}')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dHewcp-Kq0Dv"
      },
      "source": [
        "### Run Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w7meyIk2q0Dw",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol='target', maxIter=10, regParam=0.2, elasticNetParam=0.8)\n",
        "lrModel = lr.fit(train)\n",
        "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
        "print(\"Intercept: \" + str(lrModel.intercept))\n",
        "\n",
        "print(\"Root Mean Squared Error: {}\\nR Squared (R2) {}\".format(lrModel.summary.rootMeanSquaredError, lrModel.summary.r2))\n",
        "#print(f\"pValues = {lrModel.summary.pValues}\\nR2Adf = {lrModel.summary.r2adj}\")\n",
        "print(f\"R2Adf = {lrModel.summary.r2adj}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT-zwhfjJ6X2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dir(lrModel.summary))\n",
        "#print(lrModel.summary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf3wd2LSJ6X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import LinearRegressionModel\n",
        "#lrModel.write().overwrite().save('lrModel')\n",
        "lrModel2 = LinearRegressionModel.load('lrModel')\n",
        "print(lrModel2.coefficients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gDuwpgTkq0Dz"
      },
      "source": [
        "### Run test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bPWNhVvxq0D1",
        "colab": {}
      },
      "source": [
        "lrPredictions = lrModel.transform(test)\n",
        "display(lrPredictions.select(\"prediction\",\"target\",\"features\"), 30)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "lrEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\", metricName=\"r2\")\n",
        "testResult = lrModel.evaluate(test)\n",
        "print(\"Root Mean Squared Error on Test set: {}\".format(testResult.rootMeanSquaredError))\n",
        "dir(testResult)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vBJooYD3q0D5"
      },
      "source": [
        "### Try Decision Tree Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YCP7q5wdq0D7",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'target')\n",
        "dtModel = dt.fit(train)\n",
        "dtPredictions = dtModel.transform(test)\n",
        "display(dtPredictions.select(\"prediction\",\"target\",\"features\"), 30)\n",
        "important = dtModel.featureImportances\n",
        "print(type(important), important)\n",
        "#importantDict = zip(important[1], important[2])\n",
        "#print (importantDict)\n",
        "print (important[3])\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "dtEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\",metricName=\"rmse\")\n",
        "testResult = dtEvaluator.evaluate(dtPredictions)\n",
        "print(\"Root Mean Squared Error: {}\".format(testResult))\n",
        "dfML.take(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YRwC0NLEq0D-"
      },
      "source": [
        "### Try Gradient Boosted Tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lmcJ4_trq0EA",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'target', maxIter=10)\n",
        "gbtModel = gbt.fit(train)\n",
        "gbtPredictions = gbtModel.transform(test)\n",
        "display(gbtPredictions.select('prediction', 'target', 'features'), 20)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "gbtEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\",metricName=\"rmse\")\n",
        "testResult = gbtEvaluator.evaluate(gbtPredictions)\n",
        "print(\"Root Mean Squared Error: {}\".format(testResult))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rxEOP5ubq0EE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
