{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day1-IntroToSpark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRjOMwVYiaET"
      },
      "source": [
        "### The Hadoop File System (HDFS) is a distributed file system that spans across multiple nodes and saves files in a cluster. It slices large files into blocks and redundantly saves multiple copies across several nodes in the cluster according to the replication factor chosen for the cluster. \n",
        "To examine the contents of the HDFS cluster, you either need to install the Hadoop tools on a local machine or ssh into a remote machine that has them installed.\n",
        "Try the following commands to see what is currently on the cluster and add new files to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQQ_mVrQiaEW"
      },
      "source": [
        "! hadoop fs -ls /\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1g3fZTZiaEb"
      },
      "source": [
        "! hadoop fs -put /class/datasets/northwind/CSV/categories /\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2ZL2fi4iaEk"
      },
      "source": [
        "! hadoop fs -ls /\n",
        "! hadoop fs -ls /categories\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stLlpheLWf_j"
      },
      "source": [
        "### Create the Spark context to start a session and connect to the cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC4ujoSFWf_m"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/class')\n",
        "from initspark import *\n",
        "sc, spark, conf = initspark()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH-N0vuoWf_v"
      },
      "source": [
        "### Read a text file from the local file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6iEi36CWf_w"
      },
      "source": [
        "shake = sc.textFile('/class/datasets/text/shakespeare.txt')\n",
        "print(shake.count())\n",
        "print(shake.take(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdStyssVWgAU"
      },
      "source": [
        "### Parallelize will load manually created data into the spark cluster into an RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN8v44eNWgAW"
      },
      "source": [
        "r = sc.parallelize(range(1,11))\n",
        "print(r.collect())\n",
        "print(r.take(5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQVoJi3CWgAo"
      },
      "source": [
        "### Load a folder stored on HDFS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL89iGyOWgAq"
      },
      "source": [
        "cat = sc.textFile('hdfs://localhost:9000/categories')\n",
        "print(cat.collect())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlQ_B5enWgAz"
      },
      "source": [
        "### Try some different actions to fetch data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCqmMuFLWgA1"
      },
      "source": [
        "print(cat.takeOrdered(5))\n",
        "print(cat.top(5))\n",
        "print(cat.takeSample(False,5))\n",
        "cat.foreach(lambda x : print(x.upper)) # does not display properly in notebook\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqD-2-05WgBC"
      },
      "source": [
        "### Save the results in an RDD to disk. Note how it makes a folder and fills it with as many files as there are nodes solving the problem. Also, you must make sure that the folder does not exist or it throws an exception."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au3L8fPDWgBE"
      },
      "source": [
        "! rm -r /class/file1.txt\n",
        "cat.saveAsTextFile('hdfs://localhost:9000/file1.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA9xkJu41Pd2"
      },
      "source": [
        "! hadoop fs -ls /file1.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt-1Pg9DWf_1"
      },
      "source": [
        "### Use the map method to apply a function call on each element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3wwsvU6Wf_2"
      },
      "source": [
        "shake2 = shake.map(str.upper)\n",
        "shake2.take(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx0qMtPyWf_9"
      },
      "source": [
        "### Using the split method you get a list of lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szr74Z3AWf__"
      },
      "source": [
        "shake3 = shake.map(lambda x : x.split(' '))\n",
        "shake3.take(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-L0L9BiWgAF"
      },
      "source": [
        "### The flatMap method flattens the inner list to return one big list of strings instead."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP2oWr9CWgAI"
      },
      "source": [
        "shake4 = shake.flatMap(lambda x : x.split(' '))\n",
        "shake4.take(20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK88KrBTWgBN"
      },
      "source": [
        "print(cat.map(str.upper).collect())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vn43ZGHWgBT"
      },
      "source": [
        "### Parse the string into a tuple to resemble a record structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ2bIehvWgBU"
      },
      "source": [
        "cat1 = cat.map(lambda x : tuple(x.split(',')))\n",
        "cat1 = cat1.map(lambda x : (int(x[0]), x[1], x[2]))\n",
        "cat1.take(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko12EKvYWgBX"
      },
      "source": [
        "## LAB: ## \n",
        "### Put the regions folder found in /class/datasets/northwind/CSV/regions into HDFS. Read it into an RDD and convert it into a tuple shape.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Use hadoop fs -put or hdfs dfs -put\n",
        "<br>\n",
        "Read the file using sc.textFile\n",
        "<br>\n",
        "Do a map to split and another to convert the datatypes\n",
        "<br>\n",
        "<br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "! hadoop fs -put /class/datasets/northwind/CSV/regions /regions\n",
        "regions = sc.textFile('hdfs://localhost:9000/regions')\n",
        "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
        "print(regions.collect())\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZWX8p7nWgBY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc8uQPq1WgBi"
      },
      "source": [
        "### You can chain multiple transformations together to do it all in one step.\n",
        "#### Here we converted the datatypes to int, then turned the tuple into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nLPu-glWgBj"
      },
      "source": [
        "cat2 = cat.map(lambda x : tuple(x.split(','))) \\\n",
        "      .map(lambda x : (int(x[0]), x[1], x[2])) \\\n",
        "      .map(lambda x : dict(zip(['CategoryID', 'Name', 'Description'], x)))\n",
        "cat2.take(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1451bwFaWgBn"
      },
      "source": [
        "### The filter method takes a lambda that returns a True or False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQzyEYCYWgBo"
      },
      "source": [
        "cat2.filter(lambda x : x['CategoryID'] <= 5).collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22q9mGVrWgBu"
      },
      "source": [
        "### The filter expressions can be more complicated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIEc83jgWgBw"
      },
      "source": [
        "cat2.filter(lambda x : x['CategoryID'] % 2 == 0 and 'e' in x['Name']).collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-mS0TT9WgB6"
      },
      "source": [
        "### The sortBy method returns an expression that is used to sort the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQwZ4Z_vWgB7"
      },
      "source": [
        "cat2.sortBy(lambda x : x['Description']).collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4gTRsTrWgCD"
      },
      "source": [
        "### sortBy has an option ascending parameter to sort in reverse order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRq5r1UAWgCF"
      },
      "source": [
        "cat1.sortBy(lambda x : x[0], ascending = False).collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r40EeKuEWgCK"
      },
      "source": [
        "## LAB:##\n",
        "### Try to sort region in descending order by ID and then by name in ascending order. ###\n",
        "\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Use sortByKey and sortBy respectively\n",
        "<br>\n",
        "sortBy needs a lambda\n",
        "<br><br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "print(regions.sortByKey(ascending = False).collect())\n",
        "print(regions.sortBy(lambda x : x[1]).collect())\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF5E8yZyWgCL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nbV0tx11Pe_"
      },
      "source": [
        "### The following are more complex examples of using Spark to do things like JOIN and GROUP BY. For the most part these methods are replaced by the newer DataFrame methods which we will explore in the next section. We will skip a detailed explanation of the following but leave it in for self study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4e5XbSMWgCO"
      },
      "source": [
        "### Reshape categories from a tuple of three elements like (1, 'Beverages', 'Soft drinks') to a tuple with two elements (key, value) like (1, ('Beverages', 'Soft drinks'))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu2izny5WgCP"
      },
      "source": [
        "cat3 = cat1.map(lambda x : (x[0], (x[1], x[2])))\n",
        "cat3.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge3v9g1NWgCR"
      },
      "source": [
        "### The sortByKey method does not require a function as a parameter if the data is structured into a tuple of the shape (key, value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF6jtxiKWgCS"
      },
      "source": [
        "cat3.sortByKey(ascending=False).collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubk9dpCWWgCX"
      },
      "source": [
        "### Read in another CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dprx0Q-_WgCY"
      },
      "source": [
        "prod = shake = sc.textFile('/class/datasets/northwind/CSV/products')\n",
        "print(prod.count())\n",
        "prod.take(4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtLTEKPgWgCb"
      },
      "source": [
        "### Split it up and just keep the ProductID, ProductName, CategoryID, Price, Quantity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAEwAdJUWgCc"
      },
      "source": [
        "prod1 = prod.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[3]), float(x[5]), int(x[6])))\n",
        "prod1.take(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "962IynkWWgCg"
      },
      "source": [
        "### Reshape it to a key value tuple where category is the key and the other fields are the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIP64lugWgCh"
      },
      "source": [
        "prod2 = prod1.map(lambda x : (x[2], (x[0], x[1], x[3], x[4])))\n",
        "prod2.take(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1hNliYJWgCl"
      },
      "source": [
        "cat3.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS2NAAXHWgCo"
      },
      "source": [
        "### Both c3 and prod2 are in key value tuple format so they can be joined to produce a new tuple of (key, (cat, prod))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZmbDZCGWgCp"
      },
      "source": [
        "joined = cat3.join(prod2)\n",
        "joined.sortByKey().take(15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI8VffUbWgCt"
      },
      "source": [
        "## LAB: ##\n",
        "### Load territories into HDFS and join it to regions. ###\n",
        "\n",
        "\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Put /class/datasets/northwind/CSV/territories into HDFS\n",
        "<br>\n",
        "Use sc.textFile to read it into an RDD\n",
        "<br>\n",
        "Use map to split and convert it to the proper datatypes\n",
        "<br>\n",
        "Use the join method\n",
        "<br><br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "! hadoop fs -put /class/datasets/northwind/CSV/territories /\n",
        "\n",
        "territories = sc.textFile('hdfs://localhost:9000/territories')\n",
        "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
        "print(territories.collect())\n",
        "\n",
        "region_territories = regions.join(territories.map(lambda x : (x[2], (x[0],x[1]))))\n",
        "print(region_territories.collect())\n",
        "# Reshape it to make it look more normal. The * in front of the x is a python unpacking trick\n",
        "region_territories = region_territories.map(lambda x : (x[0], (x[1][0], *x[1][1])))\n",
        "print(region_territories.collect())\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG-jz9oCWgCu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7_N8D4vWgCy"
      },
      "source": [
        "### The groupBy methods are seldom used but they can produce hierarchies where children records are embedded inside a parent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8XG_Zh1WgC5"
      },
      "source": [
        "group1 = prod2.groupByKey()\n",
        "group1.take(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlfoIOfvWgCz"
      },
      "source": [
        "list(group1.take(1)[0][1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi73ZFk4WgC_"
      },
      "source": [
        "group2 = [(key, list(it)) for key, it in group1.collect()]\n",
        "for k,v in group2:\n",
        "    print ('Key:', k)\n",
        "    for x in v:\n",
        "        print(x)\n",
        "#print (group2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiQFuV5bWgDF"
      },
      "source": [
        "### The reduce methods take a function as a parameter that tells Spark how to accumulate the values for each group. The function takes two parameters; the first is the accumulated value and the second is the next value in the list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FCNryN_WgDG"
      },
      "source": [
        "shake4.map(lambda x : (x, 1)).reduceByKey(lambda x, y : x + y).sortBy(lambda x : x[1], ascending = False).take(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agTxvAFjWgDK"
      },
      "source": [
        "## LAB: ## \n",
        "### Use the territories RDD to count how many territories are in each region. \n",
        "### Display the results in regionID order and then descending order based on the counts.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Use map to put the key first then reduceByKey to accumulate the values\n",
        "<br>\n",
        "Use sortByKey to sort by regionID and sortBy with a lambda to sort by counts\n",
        "<br><br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "region_count = territories.map(lambda x : (x[2], 1)).reduceByKey(lambda x, y: x + y)\n",
        "print(region_count.sortByKey().collect())\n",
        "print(region_count.sortBy(lambda x : x[1], ascending = False).collect())\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF1xAAZVWgDL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GejVQTZMWgDP"
      },
      "source": [
        "### In this example, we are adding up all the prices for each categoryID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTQCEouBWgDQ"
      },
      "source": [
        "red1 = prod2.map(lambda x : (x[0], x[1][2])).reduceByKey(lambda x, y: x + y)\n",
        "red1.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH_hY2_PWgDV"
      },
      "source": [
        "### To accumulate more than one value, use a tuple to hold as many values as you want to aggregate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtQX34KiWgDW"
      },
      "source": [
        "red1 = prod2.map(lambda x : (x[0], (x[1][2], x[1][3], 1))).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
        "red1.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e6do4WZWgDg"
      },
      "source": [
        "### Some Python magic can make things easier in the long run.\n",
        "Named tuples make accessing the elements of the row easier.\n",
        "Unpacking using the * is a neat Python trick that is widely used.\n",
        " \n",
        "datetime has function to convert a string into a date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoiIjU2wWgDl"
      },
      "source": [
        "mort = sc.textFile('/class/datasets/finance/30YearMortgage.csv')\n",
        "head = mort.first()\n",
        "mort = mort.filter(lambda x : x != head)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fyvY687WgDp"
      },
      "source": [
        "from datetime import date, datetime\n",
        "from collections import namedtuple\n",
        "Rate = namedtuple('Rate','date fed_fund_rate avg_rate_30year')\n",
        "mort1 = mort.map(lambda x : Rate(*(x.split(','))))\n",
        "mort2 = mort1.map(lambda x : Rate(datetime.strptime(x.date, '%Y-%m').date(), float(x.fed_fund_rate), float(x.avg_rate_30year)))\n",
        "mort2.take(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i7WOKwIWgDt",
        "scrolled": true
      },
      "source": [
        "mort2.filter(lambda x : x.fed_fund_rate > .1 ).collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjXdMjdxWgDw"
      },
      "source": [
        "### HOMEWORK:\n",
        "1. The creditcard.csv dataset provides sample data on credit card transactions.\n",
        "2. Load the file into HDFS.\n",
        "3. Load the file into an RDD.\n",
        "4. Parse the file into a tuple or namedtuple or dictionary.\n",
        "5. Make sure to convert columns to the right data types.\n",
        "6. You can ignore any columns you don’t need for the solution.\n",
        "7. Filter the data to show only transactions made by women.\n",
        "8. Calculate the amount spent in each city.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQNAL1_xWgDx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}