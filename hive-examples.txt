# Make sure Hadoop is running by running start-hadoop
# If anything is corrupt run format-namenode

# Put the regions folder into HDFS
hadoop fs -put /class/datasets/northwind/CSV/regions /regions

# Start Hive CLI
hive

# Create a table 
CREATE TABLE regions(regionid int, regionname string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE
LOCATION '/regions';

# Query the table
SELECT * FROM regions;

# Drop the table
DROP table regions;

# Note the HDFS folder is deleted also
hadoop fs -ls /

# Recreate the table, this time with no location
CREATE TABLE regions(regionid int, regionname string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;

# Use the dfs command to see where it created it
dfs -ls /user/hive/warehouse ;

dfs -ls /user/hive/warehouse/regions ;

# Select from it and note it's empty
SELECT * FROM regions;

# Let's put data into the folder
! hadoop fs -put /class/datasets/northwind/CSV/regions/* /user/hive/warehouse/regions ;

# Select from it and now there's data
SELECT * FROM regions;

# Let's drop the table again and confirm the regions folder is gone
DROP table regions;

dfs -ls /user/hive/warehouse ;

# Create a northwind database
create database northwind;

use northwind;

# Recreate regions in the northwind database
CREATE TABLE regions(regionid int, regionname string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;

# Confirm the new folder structure
dfs -ls /user/hive/warehouse ;

dfs -ls /user/hive/warehouse/northwind.db ;

# Instead of putting the files with hadoop fs commands, try a new way
# Let's also try a file with headers in first row
LOAD DATA LOCAL INPATH '/class/datasets/northwind/CSVHeaders/regions' OVERWRITE INTO TABLE regions;

SELECT * FROM regions;

# Note how we have an extra row since the first line is treated as a data row
# We can eliminate it with a WHERE clause
SELECT * FROM regions WHERE regionid IS NOT NULL;

# Sometimes people prefer to use TBLPROPERTIES
CREATE EXTERNAL TABLE regions2(regionid int, regionname string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
LOCATION '/user/hive/warehouse/classroom.db/regions'
TBLPROPERTIES("skip.header.line.count"="1");

SELECT * FROM regions2;

# Use DESCRIBE to learn a little about the tables
DESCRIBE regions;

DESCRIBE FORMATTED regions;

DESCRIBE FORMATTED regions2;

# The TBLPROPERTIES skipped the first row of the file, 
# We defined a second table definition pointing to the same location 
# as the other table, but EXTERNAL means don't delete the files when 
# we DROP the table
DROP TABLE regions2;

# Could also create a view to clean up anything in the table
CREATE VIEW regions1 AS SELECT * FROM regions where regionid is not null and regionname is not null;

SELECT * FROM regions1;

# Try to create a territories table from the TSV folder
# In this case there are tab separators (\t) but otherwise the same as CSV
# The columnes are territoryid int, territoryname string, regionid int
# create the table and populate it with data using whatever method you prefer.














CREATE TABLE territories(
territoryid string,
territoryname string,
regionid int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

LOAD DATA LOCAL INPATH '/class/datasets/northwind/TSV/territories' OVERWRITE INTO TABLE territories;

# Try some more complex queries like a JOIN and GROUP BY
SELECT * FROM regions AS r JOIN territories AS t ON r.regionid = t.regionid;

SELECT regionid, COUNT(*) FROM territories GROUP BY regionid;

# It takes a lot longer because it has to do not only MAP operations but also
# SHUFFLE and REDUCE operations.
# Launch another terminal window and run spark-sql, the run the following queries
USE northwind;

SELECT * FROM regions AS r JOIN territories AS t ON r.regionid = t.regionid;

SELECT regionid, COUNT(*) FROM territories GROUP BY regionid;

# They run a lot faster because Spark is a better execution engine.
# The table definitions are stored in the metastore inside MySQL.
# Hive CLI simply uses the MapReduce Engine which is slow, 
# whereas spark-sql uses the spark engine which is faster.
# For the most part they are completely code compatible but there are some 
# extensions to the Spark syntax and a few things you do differently here and there.

# This could also be controlled in the script with the following commands:
set hive.execution.engine=mr;
set hive.execution.engine=spark;
# but for some reason the spark engine isn't working right on this particular 
# VM when using the Hive CLI, so we can just switch between the two different 
# CLI windows

# What happens if we put bad data into a folder. Take a look at this file
!cat /class/regions2.csv;

LOAD DATA LOCAL INPATH '/class/regions2.csv' INTO TABLE regions;

SELECT * FROM regions;

# Note how the extra column for row 6 is ignored
# row 7 had nothing after the comma so its regionname is an empty string
# row 8 has an empty string and ignores the third column
# row 9 didn't even have a comma so it has NULL for the regionname
# row 10 the first column would not parse to int so NULL was returned
# row 11 is a phantom row from the final \n at the end of the file

# How about other file formats. Use the Hive CLI and make sure to be in northwind
CREATE TABLE usstates
(stateid int
,statename string
,stateabbr string
,stateregion string
)
STORED AS AVRO;

LOAD DATA LOCAL INPATH '/class/datasets/northwind/AVRO/usstates' OVERWRITE INTO TABLE usstates;

SELECT * FROM usstates;

# We could ignore fields we don't want. AVRO uses the field names inside the inferSchema
# in the file header to determine what to match to your table definition
CREATE TABLE usstates2
(state_id int
,stateabbr string
,statename string
)
STORED AS AVRO;

LOAD DATA LOCAL INPATH '/class/datasets/northwind/AVRO/usstates' OVERWRITE INTO TABLE usstates2;

SELECT * FROM usstates2;

# So if you put a column name that doesn't match to what's in the interal schema 
# of the file it doesn't match
CREATE TABLE shippers
(shipperid int
,company string
,phone string
)
STORED AS AVRO;

LOAD DATA LOCAL INPATH '/class/datasets/northwind/AVRO/shippers' OVERWRITE INTO TABLE shippers;

SELECT * FROM shippers;

# Let's drop it and create it properly. 
DROP TABLE shippers;

CREATE EXTERNAL TABLE shippers
(shipperid int
,companyname string
,phone string
)
STORED AS AVRO;

LOAD DATA LOCAL INPATH '/class/datasets/northwind/AVRO/shippers' OVERWRITE INTO TABLE shippers;

SELECT * FROM shippers;

# Parquet format is similar, except the data is stored as columns instead of rows
# sometimes this is more efficient.
CREATE TABLE categories
(categoryid int
,categoryname string
)
STORED AS PARQUET;

LOAD DATA LOCAL INPATH '/class/datasets/northwind/PARQUET/categories' OVERWRITE INTO TABLE categories;

SELECT * FROM categories;

# ORC is also a column store and the syntax is similar
CREATE TABLE products
(productid int
,productname string
,categoryid int
,unitprice decimal(5,2)
,unitsinstock int
)
STORED AS ORC;

LOAD DATA LOCAL INPATH '/class/datasets/northwind/ORC/products' OVERWRITE INTO TABLE products;

SELECT * FROM products;

# We can also copy the results of a SQL query to another table using the 
# familiar CREATE TABLE AS command
CREATE TABLE shippers2
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' 
LOCATION '/shippers2'
AS
SELECT * FROM shippers;

dfs -cat /shippers2/* ;

# But what if we want the results of a query but don't want the metadata saved
# in the Hive metastore
INSERT OVERWRITE DIRECTORY '/shippers3' 
SELECT * FROM shippers;

dfs -cat /shippers3/*;

# That stored things in Hive native format, but we could add in additional clauses
# just like in CREATE TABLE, to control the output format
INSERT OVERWRITE DIRECTORY '/shippers4' 
ROW FORMAT DELIMITED FIELDS TERMINATED BY ';' 

SELECT * FROM shippers;

dfs -cat /shippers4/*;

# Or copy it directly to a local directory
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/shippers5' 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '!' 

SELECT * FROM shippers;

! cat /tmp/shippers5;

# We could also append the results of a query to an existing table
CREATE EXTERNAL TABLE shippers6
(shipperid int
,companyname string
,phone string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ':' 
LOCATION '/shippers6';

INSERT INTO shippers6 SELECT * FROM shippers;

dfs -cat /shippers6/* ;

# JSON is a bit trickier in Hive. We must load an external library and
# use an awkward SerDe name
ADD JAR /usr/local/hive/hcatalog/share/hcatalog/hive-hcatalog-core.jar;

CREATE EXTERNAL TABLE regions_json
(regionid int,
regiondescription string)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
LOCATION '/regions_json';

LOAD DATA LOCAL INPATH '/class/datasets/northwind/JSON/regions' OVERWRITE INTO TABLE regions_json;

# Note the JSON file we were given called it regionsdescription so just like ORC
# Parquet and AVRO we need to go with the schema information contained in the files
# but we could always remap it with a view afterwards
CREATE VIEW regions_json2 AS SELECT regionid, regiondescription AS regionname FROM regions_json;

select * from regions_json2;

# To just export a result to JSON use INSERT OVERWRITE DIRECTORY with some specs
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/territories6'
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
SELECT territoryid, territoryname FROM territories WHERE regionid = 1;

! cat /tmp/territories6/* ;
# but the key names are off, this doesn't work well here but in pyspark it's

# Unfortunately JSON tables defined in the Hive catalog don't work in spark sql,
# so JSON isn't a good format to use for long term storage for a lot of reasons.
# However, we can still use JSON in spark sql, it's just different syntax
# Note we don't even need to give it a schema when we do this. Spark is smart
# enough to dynamically infer the schema
CREATE TEMPORARY VIEW regions_json3
USING org.apache.spark.sql.json
OPTIONS (
  path "/regions_json"
);

SELECT * FROM regions_json3;

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/regions_json'
USING org.apache.spark.sql.json
SELECT * FROM regions_json3;

# run in terminal not spark-sql
cat /tmp/regions_json

# We can do the same for Parquet. Note here the parquet file we stored has the 
# description column we ignored when we made the table earlier, but the dynamic
# schema resolution detected it because we used TEMPORARY VIEW instead of making
# a table
CREATE TEMPORARY VIEW categories0
USING org.apache.spark.sql.parquet
OPTIONS (
  path "/user/hive/warehouse/northwind.db/categories"
);

SELECT * FROM categories0;

# ORC is easy too
CREATE TEMPORARY VIEW products0
USING ORC
OPTIONS (
  path "/user/hive/warehouse/northwind.db/products"
);

SELECT * FROM products0;

# AVRO has to be weird here and we can't use it directly in the spark-sql CLI
# but when we look at pyspark we will see it's still usable


# Here's a trick for only reading the columns we want from CSV format.
# Create a table to map up to the last column we want, we can use a fake name
# for columns we don't want
CREATE TABLE products3(
  productid int,
  productname string,
  ignore1 string, 
  categoryid int,
  ignore2 string,
  price float 
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
LOAD DATA LOCAL INPATH '/class/datasets/northwind/CSV/products' overwrite into table products3;

# Then we can create a view to fix it up and ignore rows and columns we don't want
CREATE VIEW products4 AS SELECT productid, productname, categoryid, price FROM products3 WHERE productid IS NOT NULL;

SELECT * FROM products4;

# Partitioning tables can greatly improve performance
CREATE TABLE transactions
(id int,
amount double) 
PARTITIONED BY (year int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' 
LOCATION '/transactions';

INSERT INTO transactions PARTITION(year=2015) 
SELECT 1, 100 UNION ALL SELECT 2, 200;

SHOW PARTITIONS transactions;

INSERT INTO transactions PARTITION(year=2016) SELECT 3, 300 UNION ALL SELECT 4, 400;
SHOW PARTITIONS transactions;

ALTER TABLE transactions ADD partition (year=2017);
SHOW PARTITIONS transactions;
INSERT INTO transactions PARTITION(year=2017) SELECT 5, 500 UNION ALL SELECT 6, 600;

ALTER TABLE transactions DROP PARTITION (year=2015);

# We could also partition on multiple columns and we get a nested directory structure
CREATE TABLE transactions2
(id int,
amount double) PARTITIONED BY (year int, month int)
LOCATION '/transactions2';

ALTER TABLE transactions2 ADD PARTITION (year=2015,month=1);
dfs -ls /transactions2 ;
dfs -ls /transactions2/year=2015 ;

# Let's build a partitioned table from some existing data
CREATE TABLE orders_table (
    orderid smallint,
    customerid varchar(5),
    skip0 char(1),
    orderdate date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/orders_table';

LOAD DATA LOCAL INPATH '/class/datasets/northwind/CSVHeaders/orders' overwrite into table orders_table;

# This view will create a year column for us to partition on
CREATE VIEW orders_view AS 
SELECT orderid, customerid, orderdate, cast(substring(cast(orderdate as string), 1, 4) as int) AS year 
FROM orders_table
WHERE orderid IS NOT NULL AND customerid IS NOT NULL;

# Now let's build a partitioned table
CREATE EXTERNAL TABLE orders_part (
    orderid smallint,
    customerid varchar(5),
    orderdate date)
PARTITIONED BY(year int)
STORED AS PARQUET
LOCATION 'hdfs://localhost:9000/orders_part';

SHOW PARTITIONS orders_part;

# First we will insert the data into a partition at a time being careful to 
# use a WHERE clause and provide the correct partition to add the data to
INSERT OVERWRITE TABLE orders_part PARTITION (year = 1996)
SELECT orderid, customerid, orderdate FROM orders_view WHERE year(orderdate) = 1996;

INSERT INTO orders_part PARTITION (year = 1997)
SELECT orderid, customerid, orderdate FROM orders_view WHERE year(orderdate) = 1997;

INSERT OVERWRITE TABLE orders_part PARTITION (year = 1998)
SELECT orderid, customerid, orderdate FROM orders_view WHERE year(orderdate) = 1998;

# run this in spark-sql to see how many records we have in each year
SELECT year, count(*) FROM orders_part GROUP BY year;

# Now let's drop a partition, but because it's an external table it doesn't
# delete the files
ALTER TABLE orders_part DROP PARTITION (year=1998);

SELECT year, count(*) FROM orders_part GROUP BY year;

# So if we add the partition back the files are already there
ALTER TABLE orders_part ADD PARTITION (year=1998);

SELECT year, count(*) FROM orders_part GROUP BY year;

# Let's do this again but this time using dynamic partitioning
drop table orders_part;
dfs -rm -r /orders_part;

CREATE EXTERNAL TABLE orders_part (
    orderid smallint,
    customerid varchar(5),
    orderdate date)
PARTITIONED BY(year int)
STORED AS PARQUET
LOCATION 'hdfs://localhost:9000/orders_part';

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

INSERT OVERWRITE TABLE orders_part PARTITION (year)
SELECT orderid, customerid, orderdate, year FROM orders_view;

SELECT year, COUNT(*) FROM orders_part GROUP BY year;

dfs -mv /orders_part/year=1998 /orders1998 ;
# we moved the file so we get an error when we try to run the SELECT
SELECT year, COUNT(*) FROM orders_part GROUP BY year;

ALTER TABLE orders_part DROP PARTITION(year=1998);


# MySQL has a group_concat function that can combine a string column from multiple
# rows into a bigger string 
select categoryid, count(*) from products group by categoryid;
select categoryid, group_concat(productname) from products group by categoryid;

# Hive has a similar idea but instead we can combine them into an array object
# These are best run in spark-sql for performance
SELECT regionid, COLLECT_LIST(territoryname) AS territorylist
FROM territories GROUP BY regionid;

# We could get a single string from the array if we want
SELECT regionid, CONCAT_WS(', ', COLLECT_LIST(territoryname)) AS territorylist
FROM territories GROUP BY regionid;

# Turn the results into a table
CREATE TABLE territory_list as
SELECT regionid, COLLECT_LIST(territoryname) AS territorylist
FROM territories GROUP BY regionid;

DESCRIBE territory_list;
! hadoop fs -cat /user/hive/warehouse/northwind.db/territory_list/*

# That's fine for an array of one single column but what about a more complex shape
SELECT regionid, COLLECT_LIST(NAMED_STRUCT('territoryid', territoryid, 'territoryname', territoryname)) AS territorylist
FROM territories GROUP BY regionid;

# What if we want the sub lists sorted, we can use a WITH clause and some new commands
WITH tmp AS (
  SELECT * FROM territories DISTRIBUTE BY regionid SORT BY territoryid
)
SELECT regionid, COLLECT_LIST(NAMED_STRUCT('territoryid', territoryid, 'territoryname', territoryname)) AS territorylist
FROM tmp 
GROUP BY regionid;

WITH tmp AS (
  SELECT * FROM territories DISTRIBUTE BY regionid SORT BY territoryname
)
SELECT regionid, COLLECT_LIST(NAMED_STRUCT('territoryid', territoryid, 'territoryname', territoryname)) AS territorylist
FROM tmp 
GROUP BY regionid;

# Let's do a join and collect all together
CREATE TABLE region_territory AS
SELECT r.regionid, r.regionname, COLLECT_LIST(NAMED_STRUCT('territoryid', t.territoryid, 'territoryname', t.territoryname)) AS territorylist
FROM territories AS t
JOIN regions AS r ON t.regionid = r.regionid
GROUP BY r.regionid, r.regionname;

SELECT * FROM region_territory;

# What if you have this nested repeating struture and want to return it to flat
# denormalized shape. Since each element of territorylist field is a string
# the alias territory refers to an individual string field
SELECT regionid, territory
FROM territory_list LATERAL VIEW EXPLODE(territorylist) AS territory;

# But for this table where we have each element as a structure, then each tab
# element is an individual structure object
SELECT regionid, regionname, t
FROM region_territory LATERAL VIEW EXPLODE(territorylist) AS t;

# But we can flatten it out but using dot syntax similar to field names in OO languages
SELECT regionid, regionname, t.territoryid, t.territoryname
FROM region_territory LATERAL VIEW EXPLODE(territorylist) AS t;

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/region_list'
USING org.apache.spark.sql.json
SELECT regionid, collect_list(territoryname) AS territories
FROM territories
GROUP BY regionid;


# More examples of nested repeatings using CREATE TABLE syntax
CREATE TABLE Person(
PersonID int,
Name string,
Skills ARRAY<string>
)
location '/person';

INSERT INTO Person 
SELECT 1, 'joey', ARRAY('Java', 'Python', 'Hadoop') 
UNION ALL SELECT 2, 'mary', ARRAY('C++', 'Java', 'Hive');


INSERT INTO Person 
SELECT 3, 'han', ARRAY('Java', 'Python', 'Java', 'C++');

# in hive
CREATE TABLE Person2(
PersonID int,
Name string,
Skills ARRAY<string>
)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
location '/person2';

INSERT INTO person2
SELECT 1, 'joey', ARRAY('Java', 'Python', 'Hadoop') 
UNION ALL SELECT 2, 'mary', ARRAY('C++', 'Java', 'Hive');
! hadoop fs -cat /person2/* ;

# In spark-sql
CREATE TABLE person3(
PersonID int,
Name string,
Skills ARRAY<string>
)
USING org.apache.spark.sql.json
OPTIONS (path "/person3");

INSERT INTO person3
SELECT 1, 'joey', ARRAY('Java', 'Python', 'Hadoop') 
UNION ALL SELECT 2, 'mary', ARRAY('C++', 'Java', 'Hive');
! hadoop fs -cat /person3/* ;

# We can use indexer notation like in Python to get elements of an array
SELECT personid, name, skills[0] AS skill1, skills[size(skills)-1] FROM person;

SELECT PersonID, Name, SkillName 
FROM Person LATERAL VIEW EXPLODE(Skills) AS SkillName;

# We could explode and flatten a nested table to dernomalized and regroup it on
# another field
WITH x AS 
(SELECT personid, name, skill 
FROM person LATERAL VIEW EXPLODE(skills) AS skill)
SELECT skill, COUNT(*) AS cnt FROM x GROUP BY skill;

# or use derived table syntax
SELECT skill, COUNT(*) AS cnt 
FROM (SELECT personid, name, skill FROM person LATERAL VIEW EXPLODE(skills) AS skill) as x 
GROUP BY skill;

# Temporary views are cool if you want to reuse a base query several times without
# having to cut and past the WITH block
CREATE TEMPORARY VIEW x AS 
SELECT personid, name, skill 
FROM person LATERAL VIEW EXPLODE(skills) AS skill;
SELECT skill, count(*) AS cnt FROM x GROUP BY skill;

# Here's how we'd create a table with a struct
CREATE TABLE complex1(
id int,
name string,
location struct<lat:decimal(4,2), lng:decimal(4,2), city:string>
);
INSERT INTO table complex1
select 1, 'joey', NAMED_STRUCT('lat', 25.81, 'lng', -80.19, 'city', 'Miami');

# But usually when we use a struct it's also an array
CREATE TABLE complex2(
id int,
name string,
locations array<struct<lat:decimal(4,2), lng:decimal(4,2), city:string>>
);

INSERT INTO table complex2
select 1, 'joey', ARRAY(
NAMED_STRUCT('lat', 25.81, 'lng', -80.19, 'city', 'Miami'),
NAMED_STRUCT('lat', 25.81, 'lng', -80.19, 'city', 'DC'));

SELECT * FROM complex2;


select id, name, l
from complex2 lateral view explode(locations) exploded_table as l;

select id, name, l.lat, l.lng, l.city
from complex2 lateral view explode(locations) exploded_table as l;

create table complex3 as
select id, name, l.lat, l.lng, l.city
from complex2 lateral view explode(locations) exploded_table as l;

select id, name, named_struct('lat', lat, 'lng', lng, 'city', city) as location
from complex3;

select id, name, collect_list(named_struct('lat', lat, 'lng', lng, 'city', city)) as locations
from complex3
group by id, name;

-----------------------------------------
clustering doesn't seem to work
# Let's try clustering
set hive.enforce.bucketing = true;

CREATE TABLE orderdetails (
    orderid smallint,
    productid smallint,
    unitprice decimal,
    quantity smallint,
    discount decimal
)
CLUSTERED BY(orderid) SORTED BY (productid) INTO 16 BUCKETS
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/orderdetails';

LOAD DATA LOCAL INPATH '/class/datasets/northwind/TSV/orderdetails' overwrite into table orderdetails;

dfs -ls /orderdetails;
dfs -cat /orderdetails/000000_0;

# Notice how it breaks up the data into 16 file based on the orderid

CREATE TABLE orderdetails1 (
    orderid smallint,
    productid smallint,
    unitprice decimal,
    quantity smallint,
    discount decimal
)
CLUSTERED BY(orderid) SORTED BY (productid) INTO 16 BUCKETS
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
LOCATION '/orderdetails1';

INSERT INTO orderdetails1 SELECT * FROM orderdetails;

dfs -ls /orderdetails1;

select count(*) from orderdetails0 TABLESAMPLE (BUCKET 8 OUT OF 16);
select count(*) from orderdetails0 TABLESAMPLE (50 PERCENT);
--------------------------------
