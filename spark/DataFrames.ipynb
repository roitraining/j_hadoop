{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2-DataFrames.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JdIfR70Jxu2"
      },
      "source": [
        "### Set up the Spark environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os_Mz8CUJxu5"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/class')\n",
        "from initspark import *\n",
        "sc, spark, conf = initspark()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vl7-cmUJxu-"
      },
      "source": [
        "### Turn a simple RDD into a DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sou-X5kCJxu_",
        "scrolled": true
      },
      "source": [
        "x = sc.parallelize([(1,'alpha'),(2,'beta')])\n",
        "x0 = spark.createDataFrame(x)\n",
        "x0.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_1Cg_xuJxvE"
      },
      "source": [
        "### Give the DataFrame meaningful column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udSoOBO5JxvF"
      },
      "source": [
        "x1 = spark.createDataFrame(x, schema=['ID','Name'])\n",
        "x1.show()\n",
        "print(x1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiGdbXBVJxvJ"
      },
      "source": [
        "### Give a DataFrame a schema with column names and data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en8OdGGlJxvK"
      },
      "source": [
        "x2 = spark.createDataFrame(x, 'ID:int, Name:string')\n",
        "x2.show()\n",
        "print(x2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex6IIfYYJxxp"
      },
      "source": [
        "### Create a schema object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEYmzqk8Jxxq"
      },
      "source": [
        "schema1 = StructType([\n",
        "    StructField('ID', IntegerType()), \n",
        "    StructField('Name', StringType())\n",
        "])\n",
        "x3 = spark.createDataFrame(x, schema = schema1)\n",
        "x3.show()\n",
        "print(x3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT2o_wyuJxvi"
      },
      "source": [
        "### The built in toDF method does the same thing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdjwqSI6Jxvj",
        "scrolled": true
      },
      "source": [
        "x.toDF().printSchema()\n",
        "x.toDF(['ID', 'Name']).printSchema()\n",
        "x.toDF('ID:int, Name:string').printSchema()\n",
        "x.toDF(schema = schema1).printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdAF7yVmJxvy"
      },
      "source": [
        "## LAB: ## \n",
        "### Use the regions and territories RDDs from the previous lab and convert them into DataFrames with meaningful schemas.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Use sc.textFile to read the files\n",
        "<br>\n",
        "Use map functions to split and convert the data\n",
        "<br>\n",
        "Use spark.createDataFrame and toDF to convert RDD into DataFrames\n",
        "<br>\n",
        "<br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "regions = sc.textFile('hdfs://localhost:9000/regions')\n",
        "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
        "regionsdf = spark.createDataFrame(regions, 'RegionID:int, RegionName:string')\n",
        "regionsdf.show()\n",
        "\n",
        "territories = sc.textFile('hdfs://localhost:9000/territories')\n",
        "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
        "territoriesdf = territories.toDF('TerritoryID:int, TerritoryName:string, RegionID: int')\n",
        "territoriesdf.show()\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIbiLKz0Jxvz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5J3KIQfJxw5"
      },
      "source": [
        "### Examples of reading a CSV directly into a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJWVKyXeJxw6"
      },
      "source": [
        "filename = '/class/datasets/northwind/CSV/categories'\n",
        "cat1 = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = False)\n",
        "cat1.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgxblgrarL18"
      },
      "source": [
        "cat1 = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = True)\n",
        "cat1.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_er2IqEMrL2A"
      },
      "source": [
        "### There are several alternate syntaxes which can be confusing, but since you will encounter them, you need to learn to recognize the different options.\n",
        "option and options allow you pass parameters in different ways, but not the true is quoted and lowercase because it is a java value, but you could also pass it as a True Python value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmIrAg1KJxxA"
      },
      "source": [
        "cat2 = spark.read.format('csv').option('header','true').option('inferSchema','true').load(filename)\n",
        "cat2.printSchema()\n",
        "cat2 = spark.read.format('csv').options(header=True, inferSchema='true').load(filename)\n",
        "cat2.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtgs8nowrL2F"
      },
      "source": [
        "### If there is a top-level read function for the file type you want, that's the cleanest option, and pass in the parameters as named parameters. Not all formats have this and also legacy code written before this may use the old style syntax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQVzZUyVJxxE"
      },
      "source": [
        "cat3 = spark.read.csv(filename, header = True, inferSchema = True)\n",
        "cat3.printSchema()\n",
        "cat3.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCU_UT9UrL2L"
      },
      "source": [
        "### As the tables get more complex, there is a Jupyter command that will show the tables in a prettier format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDAgmHbOrL2M"
      },
      "source": [
        "display(cat3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvxqsBmgrL2R"
      },
      "source": [
        "## LAB: ## \n",
        "### Load the products table using any of the spark.read methods.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Use spark.read.csv\n",
        "<br>\n",
        "Make sure to read the version that has headers if you want to infer schema\n",
        "<br><br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "prod1 = spark.read.csv('/class/datasets/northwind/CSVHeaders/products', header=True, inferSchema=True)\n",
        "prod1.printSchema()\n",
        "display(prod1)```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HQkI0drL2S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9nYFZdErL2W"
      },
      "source": [
        "### Using a schema is a good idea for performance if you know what it is. Usually you can infer schema during development and use it as a helper to build the schema to use for production."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzaLUc5frL2X"
      },
      "source": [
        "\n",
        "prodSchema = StructType([\n",
        "    StructField('ProductID', IntegerType()), \n",
        "    StructField('ProductName', StringType()),\n",
        "    StructField('SupplierID', IntegerType()), \n",
        "    StructField('CategoryID', IntegerType()), \n",
        "    StructField('QuantityPerUnit', StringType()), \n",
        "    StructField('UnitPrice', FloatType()), \n",
        "    StructField('UnitsInStock', IntegerType()), \n",
        "    StructField('UnitsOnOrder', IntegerType()), \n",
        "    StructField('ReorderLevel', IntegerType()), \n",
        "    StructField('Discontinued', IntegerType())\n",
        "])\n",
        "\n",
        "prod2 = spark.read.csv('/class/datasets/northwind/CSVHeaders/products', header=True, schema=prodSchema)\n",
        "print(prod2)\n",
        "display(prod2)\n",
        "\n",
        "# prod2 = spark.read.csv('/class/datasets/northwind/CSVHeaders/products', header=True, inferSchema=False)\n",
        "# print(prod2)\n",
        "# prodSchema2 = \"ProductID:int, ProductName:string, SupplierID:int, CategoryID:int, QuantityPerUnit:string, UnitPrice:double, UnitsInStock:int, UnitsOnOrder:int, ReorderLevel:int, Discontinued:int\"\n",
        "\n",
        "# prod3 = prod2.toDF(prodSchema2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqpW0HPKJxv2"
      },
      "source": [
        "### Convert a DataFrame into a JSON string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I64RwvUdJxv3"
      },
      "source": [
        "print (cat2.toJSON().take(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn23EMQcrL2h"
      },
      "source": [
        "### JSON is another top-level supported format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypuDVM-crL2i"
      },
      "source": [
        "cat4 = spark.read.json('/class/datasets/northwind/JSON/categories')\n",
        "display(cat4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upJodEzYrL2p"
      },
      "source": [
        "### You can also use schemas, but be careful of case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUo_rYPIrL2t",
        "scrolled": true
      },
      "source": [
        "prod = spark.read.json('/class/datasets/northwind/JSON/products', schema=prodSchema)\n",
        "display(prod)\n",
        "\n",
        "prodSchema = StructType([\n",
        "    StructField('productid', IntegerType()), \n",
        "    StructField('productname', StringType()),\n",
        "    StructField('supplierid', IntegerType()), \n",
        "    StructField('categoryid', IntegerType()), \n",
        "    StructField('quantityperunit', StringType()), \n",
        "    StructField('unitprice', FloatType()), \n",
        "    StructField('unitsinstock', IntegerType()), \n",
        "    StructField('unitsonorder', IntegerType()), \n",
        "    StructField('reorderlevel', IntegerType()), \n",
        "    StructField('discontinued', IntegerType())\n",
        "])\n",
        "prod = spark.read.json('/class/datasets/northwind/JSON/products', schema=prodSchema)\n",
        "display(prod)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJAzAmRjrL2z"
      },
      "source": [
        "### You may also see the older style syntax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Lh5zjLrL20"
      },
      "source": [
        "prod = spark.read.format('json').load('/class/datasets/northwind/JSON/products')\n",
        "display(prod)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bky6-O0IrL23"
      },
      "source": [
        "### Writing a DataFrame uses a similar syntax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laWiPGfQrL24"
      },
      "source": [
        "! rm -r /tmp/prodjson\n",
        "prod.write.json('/tmp/prodjson')\n",
        "! cat /tmp/prodjson/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtB0MwNcrL27"
      },
      "source": [
        "! rm -r /tmp/prodcsv\n",
        "prod.write.csv('/tmp/prodcsv', sep = '\\t', header=True)\n",
        "! cat /tmp/prodcsv/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFGU69hUrL3B"
      },
      "source": [
        "### Note the use of mode('overwrite') here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek3P9sg0rL3C"
      },
      "source": [
        "prod.write.mode('overwrite').orc('/tmp/prodorc')\n",
        "! cat /tmp/prodorc/*\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHHR9zE1rL3G"
      },
      "source": [
        "prod.write.mode('overwrite').parquet('/tmp/prodparquet')\n",
        "! cat /tmp/prodparquet/*\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diq6HSPzrL3J"
      },
      "source": [
        "### AVRO is a little different, it is built in now but doesn't have a top-level method for it, so you need to use the old style syntax.\n",
        "This doesn't always work inside of a notebook either, so take a look at the program and run it from spark-submit with the proper package dependency added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "954CEsFMrL3K"
      },
      "source": [
        "! cat /class/avro.py\n",
        "\n",
        "! spark-submit --packages org.apache.spark:spark-avro_2.11:2.4.3 /class/avro.py\n",
        "        \n",
        "#prod4.write.format(\"avro\").mode('overwrite').save('/tmp/prodavro')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDrJv-aZrL3N"
      },
      "source": [
        "## LAB: ## \n",
        "### Try to read in a few files with different formats and write them out to other formats. \n",
        "### Read shippers found in TSV and write it out as JSON.\n",
        "### Read orders found in CSVHeaders and write it out as ORC.\n",
        "### Read orderdetails found in JSON and write it out as Parquet.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Use the syntax that is easiest for you\n",
        "<br>\n",
        "If there is a top level function for the file format, that is usually the easiest way\n",
        "<br>\n",
        "TSV is just the same as CSV, but if there are no headers you need to supply a schema\n",
        "<br>\n",
        "Remember to either remove the destination folder before writing or use an overwrite option\n",
        "<br><br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "shipperSchema = StructType([\n",
        "    StructField('shipperid', StringType()), \n",
        "    StructField('shippername', StringType()),\n",
        "    StructField('phone', StringType())\n",
        "])\n",
        "\n",
        "shippers = spark.read.csv('/class/datasets/northwind/TSV/shippers', sep='\\t', header=False, inferSchema=False, schema=shipperSchema)\n",
        "shippers.write.mode('overwrite').json('/tmp/shippersjson')\n",
        "\n",
        "orders = spark.read.csv('/class/datasets/northwind/CSVHeaders/orders', header=True, inferSchema=True)\n",
        "orders.write.mode('overwrite').orc('/tmp/ordersorc')\n",
        "\n",
        "orderdetails = spark.read.json('/class/datasets/northwind/JSON/orderdetails')\n",
        "orderdetails.write.mode('overwrite').parquet('/tmp/orderdetailsparquet')\n",
        "\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTe379JLrL3O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7TI07ZPrL3X"
      },
      "source": [
        "### JDBC and NoSQL sources can also be used, but we will explore those options in the next session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0JDyefiJxv7"
      },
      "source": [
        "prod.printSchema()\n",
        "print (prod.columns, prod.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2PVAJGSJxv-"
      },
      "source": [
        "### Choose particular columns from a DataFrame.\n",
        "You can use quoted strings for the column names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shp65QEQJxv_"
      },
      "source": [
        "display(prod.select('productid', 'productname', 'unitprice'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TuZyScErL3i"
      },
      "source": [
        "### Or you can use a pythonic syntax using the DataFrame name and field name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U96JctQ0rL3i"
      },
      "source": [
        "display(prod.select(prod.productid, prod.productname, prod.unitprice))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D_4iv3WrL3l"
      },
      "source": [
        "### Case is ignored if you use quoted strings but not if you use python syntax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFILXweIrL3m"
      },
      "source": [
        "display(prod.select('Productid', 'productname', 'unitprice'))\n",
        "display(prod.select(prod.Productid, prod.productname, prod.unitprice))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTcz7pkirL3p"
      },
      "source": [
        "### Distinct is a method after the select method chooses the columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOXLR2IPJxwD"
      },
      "source": [
        "display(prod.select('CategoryID').distinct())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9NySqdqJxwH"
      },
      "source": [
        "### Sort a DataFrame. The sort and orderBy methods are different aliases for the exact same method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9IkEP-4JxwI"
      },
      "source": [
        "display(prod.sort(prod.unitprice))\n",
        "display(prod.orderBy('unitprice', ascending = False))\n",
        "display(prod.select('productid', 'productname', 'unitprice').orderBy('unitprice'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6zM9-5MJxwS"
      },
      "source": [
        "### Create a new DataFrame with a new calculated column added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfXZYr82JxwU"
      },
      "source": [
        "prod2 = prod.withColumn('value', prod.unitprice * prod.unitsinstock)\n",
        "display(prod2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xImpvfmVJxwb"
      },
      "source": [
        "### Remove an unwanted column from a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvbveAjfJxwd",
        "scrolled": true
      },
      "source": [
        "prod2 = prod2.drop('quantityperunit')\n",
        "display(prod2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHhY9u9Jxwh"
      },
      "source": [
        "### The filter and where methods can both be used and have alternative ways to represent the condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bg7uhoEJxwj",
        "scrolled": true
      },
      "source": [
        "p = prod\n",
        "display(p.filter(p.unitprice > 100))\n",
        "display(p.filter('unitprice > 100'))\n",
        "# Note == when using python syntax\n",
        "display(p.where(p.categoryid == 2))\n",
        "# Note = when using quoted SQL like syntax\n",
        "display(p.where('categoryid = 2'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUnUFFZXrL3-"
      },
      "source": [
        "### More complex conditions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_MDNTlbrL4A",
        "scrolled": false
      },
      "source": [
        "display(p.where('unitprice >= 50 and unitprice <= 100'))\n",
        "display(p.where('unitprice between 50 and 100'))\n",
        "\n",
        "display(p.where((p.unitprice >=50) & (p.unitprice <= 100)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5E4afFTJxwp"
      },
      "source": [
        "## LAB: ## \n",
        "### Find all the products in category 2 with fewer units in stock than units on order. \n",
        "### Only display with productid, name, unitsinstock, unitsonorder, and unitprice.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Use the where or filter method. It's probably easier to use a quoted SQL style syntax\n",
        "<br>\n",
        "Use select to get the columns you want to see\n",
        "<br><br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "display(p.where('unitsinstock < unitsonorder and categoryid = 2').select('productid','productname', 'unitsinstock', 'unitsonorder', 'unitprice'))\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kZwJv9LJxwr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRYHW5DlJxwx"
      },
      "source": [
        "### JOINs work as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLGYJT1PJxwy"
      },
      "source": [
        "tab1 = sc.parallelize([(1, 'Alpha'), (2, 'Beta'), (3, 'Delta')]).toDF('ID:int, code:string')\n",
        "tab2 = sc.parallelize([(100, 'One', 1), (101, 'Two', 2), (102, 'Three', 1), (103, 'Four', 4)]).toDF('ID:int, name:string, parentID:int')\n",
        "tab1.join(tab2, tab1.ID == tab2.parentID).show()\n",
        "tab1.join(tab2, tab1.ID == tab2.parentID, 'left').show()\n",
        "tab1.join(tab2, tab1.ID == tab2.parentID, 'right').show()\n",
        "tab1.join(tab2, tab1.ID == tab2.parentID, 'full').show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbKSw8mFJxw1"
      },
      "source": [
        "###  Examples of aggregate functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q-LPpNaJxw2",
        "scrolled": true
      },
      "source": [
        "tab3 = sc.parallelize([(1, 10), (1, 20), (1, 30), (2, 40), (2,50)]).toDF('groupID:int, amount:int')\n",
        "tab3.groupby('groupID').max().show()\n",
        "tab3.groupby('groupID').sum().show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iArjkg9ErL4b"
      },
      "source": [
        "x = tab3.groupby('groupID')\n",
        "x.agg({'amount':'sum'}).show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsi0GmVyrL4e"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "x.agg(F.sum('amount'), F.max('amount')).show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB_yhwpjrL4j"
      },
      "source": [
        "from pyspark.sql.functions import expr\n",
        "x.agg(expr('sum(amount) as total')).show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDGFU2bwJxxN"
      },
      "source": [
        "## LAB: ## \n",
        "### Join products and categories together displaying only the product and category ID's and names, sort by categoryid and productid.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Make sure not to show the common column twice\n",
        "<br>\n",
        "Select with python style makes it easier to distinguish which columns you want from a join\n",
        "<br><br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "c = cat\n",
        "p = prod\n",
        "display(c.join(p, c.categoryid == p.categoryid).select(c.categoryid, c.categoryname, p.productid, p.productname).orderBy('categoryid', 'productid'))```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTxpSZd7JxxP",
        "scrolled": true
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pxxBU4AJxxS"
      },
      "source": [
        "### Sometimes you want to just rename a column so here are two ways to accomplish that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RUqQyieJxxU"
      },
      "source": [
        "display(p.withColumnRenamed('unitprice','listprice'))\n",
        "cols = p.columns # get a list of all the current column names\n",
        "cols[5] = 'listprice' # replace a column position with the new name \n",
        "p1 = p.toDF(*cols) # create a new dataframe from the original with a list of column names\n",
        "display(p1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GudZR4A_Jxxs"
      },
      "source": [
        "## HOMEWORK: ## \n",
        "### Join Orders, OrderDetails, and Products together. Find the sales total for each category listed in descending order by sales.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Load each file into a dataframe and give them single letter aliases for simplicity\n",
        "<br>\n",
        "Join products and order details together on productid\n",
        "<br>\n",
        "Join that to orders on orderid\n",
        "<br>\n",
        "Createa a calculated column to get the line total for each order details\n",
        "<br>\n",
        "Group by categoryID and calculate the sum of the line totals \n",
        "<br>\n",
        "Sort on the calculated total\n",
        "<br><br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "prodSchema = StructType([\n",
        "    StructField('productid', IntegerType()), \n",
        "    StructField('productname', StringType()),\n",
        "    StructField('supplierid', IntegerType()), \n",
        "    StructField('categoryid', IntegerType()), \n",
        "    StructField('quantityperunit', StringType()), \n",
        "    StructField('unitprice', FloatType()), \n",
        "    StructField('unitsinstock', IntegerType()), \n",
        "    StructField('unitsonorder', IntegerType()), \n",
        "    StructField('reorderlevel', IntegerType()), \n",
        "    StructField('discontinued', IntegerType())\n",
        "])\n",
        "\n",
        "\n",
        "```\n",
        "</p>\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRz6DsSUrL4w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}