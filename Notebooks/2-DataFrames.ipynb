{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JdIfR70Jxu2"
   },
   "source": [
    "### Set up the Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "os_Mz8CUJxu5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/class')\n",
    "from initspark import *\n",
    "sc, spark, conf = initspark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vl7-cmUJxu-"
   },
   "source": [
    "### Turn a simple RDD into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sou-X5kCJxu_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(1,'alpha'),(2,'beta')])\n",
    "print(x.collect())\n",
    "x0 = spark.createDataFrame(x)\n",
    "x0.show()\n",
    "x0.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_1Cg_xuJxvE"
   },
   "source": [
    "### Give the DataFrame meaningful column names, by providing a list of names as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udSoOBO5JxvF"
   },
   "outputs": [],
   "source": [
    "x1 = spark.createDataFrame(x, schema=['ID','Name'])\n",
    "x1.show()\n",
    "print(x1)\n",
    "print(x1.collect())\n",
    "x2 = x1.take(1)\n",
    "print(x2[0])\n",
    "print(x2[0].ID, x2[0]['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiGdbXBVJxvJ"
   },
   "source": [
    "### Give a DataFrame a schema with column names and data types, by providing a single string with the names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "en8OdGGlJxvK"
   },
   "outputs": [],
   "source": [
    "x2 = spark.createDataFrame(x, schema = 'ID:int, Name:string')\n",
    "x2.show()\n",
    "print(x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex6IIfYYJxxp"
   },
   "source": [
    "### Create a schema object to be more specific. Also some functions cannot use string but require schema objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEYmzqk8Jxxq"
   },
   "outputs": [],
   "source": [
    "# CREATE TABLE schema1 (ID int, Name string);\n",
    "schema1 = StructType([\n",
    "    StructField('ID', IntegerType()), \n",
    "    StructField('Name', StringType())\n",
    "])\n",
    "x3 = spark.createDataFrame(x, schema = schema1)\n",
    "x3.show()\n",
    "print(x3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XT2o_wyuJxvi"
   },
   "source": [
    "### The built in toDF method does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdjwqSI6Jxvj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.toDF().printSchema()\n",
    "x.toDF(['ID', 'Name']).printSchema()\n",
    "x.toDF('ID:int, Name:string').printSchema()\n",
    "x.toDF(schema = schema1).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdAF7yVmJxvy"
   },
   "source": [
    "## LAB: ## \n",
    "### Use the regions and territories RDDs from the previous lab and convert them into DataFrames with meaningful schemas.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use sc.textFile to read the files\n",
    "<br>\n",
    "Use map functions to split and convert the data\n",
    "<br>\n",
    "Use spark.createDataFrame and toDF to convert RDD into DataFrames\n",
    "<br>\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "regions = sc.textFile('hdfs://localhost:9000/regions')\n",
    "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
    "regionsdf = spark.createDataFrame(regions, 'RegionID:int, RegionName:string')\n",
    "regionsdf.show()\n",
    "\n",
    "territories = sc.textFile('hdfs://localhost:9000/territories')\n",
    "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
    "territoriesdf = territories.toDF('TerritoryID:int, TerritoryName:string, RegionID: int')\n",
    "territoriesdf.show()\n",
    "\n",
    "# instead of collecting the data as a list of rows, it can be brought back as a pandas DataFrame\n",
    "regions.toPandas()\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIbiLKz0Jxvz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5J3KIQfJxw5"
   },
   "source": [
    "### Examples of reading a CSV directly into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dtgs8nowrL2F"
   },
   "source": [
    "### If there is a top level read function for the file type you want, that's the cleanest option, and pass in the parameters as named parameters. Not all formats have this and also legacy code written before this may use the old style syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQVzZUyVJxxE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename1 = 'file:///class/datasets/northwind/CSVHeaders/categories'\n",
    "cat1 = spark.read.csv(filename1, header = True, inferSchema = True)\n",
    "print(cat1)\n",
    "cat1.printSchema()\n",
    "cat1.show()\n",
    "cat1.collect()\n",
    "# CREATE TEMPORARY TABLE categories (CategoryID int, CategoryName string, Description string)\n",
    "# ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "# LOCATION 'file:///class/datasets/northwind/CSVHeaders/categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cat1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy style is still seen and uses a few alternate syntaxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJWVKyXeJxw6"
   },
   "outputs": [],
   "source": [
    "filename2 = 'file:///class/datasets/northwind/TSV/categories'\n",
    "cat2 = spark.read.load(filename2, format = 'csv', sep = '\\t', inferSchema = True, header = False)\n",
    "cat2.printSchema()\n",
    "cat2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgxblgrarL18"
   },
   "outputs": [],
   "source": [
    "filename3 = 'file:///class/datasets/northwind/CSVHeaders/categories'\n",
    "cat3 = spark.read.load(filename3, format = 'csv', sep = ',', inferSchema = True, header = True)\n",
    "cat3.printSchema()\n",
    "cat3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_er2IqEMrL2A"
   },
   "source": [
    "### There are several alternate syntaxes which can be confusing, but since you will encounter them, you need to learn to recognize the different options\n",
    "option and options allow you pass parameters in different ways, but not the true is quoted and lowercase because it is a java value, but you could also pass it as a True Python value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GmIrAg1KJxxA"
   },
   "outputs": [],
   "source": [
    "cat4 = spark.read.format('csv').option('header','true').option('inferSchema','true').load(filename1)\n",
    "cat4.printSchema()\n",
    "cat5 = spark.read.format('csv').options(header=True, inferSchema='true').load(filename1)\n",
    "cat5.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCU_UT9UrL2L"
   },
   "source": [
    "### As the tables get more complex, there is a Jupyter command that will show the tables in a prettier format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDAgmHbOrL2M"
   },
   "outputs": [],
   "source": [
    "display(cat3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvxqsBmgrL2R"
   },
   "source": [
    "## LAB: ## \n",
    "### Load the products table using any of the spark.read methods.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use spark.read.csv\n",
    "<br>\n",
    "Make sure to read the version that has headers if you want to infer schema\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "prod1 = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/products', header=True, inferSchema=True)\n",
    "prod1.printSchema()\n",
    "display(prod1)```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84HQkI0drL2S"
   },
   "outputs": [],
   "source": [
    "#prod1 = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/products', header=True, inferSchema=True)\n",
    "#prod1 = spark.read.json('file:///class/datasets/northwind/JSON/products')\n",
    "prod1 = spark.read.orc('hdfs://localhost:9000/products')\n",
    "prod1.printSchema()\n",
    "display(prod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9nYFZdErL2W"
   },
   "source": [
    "### Using a schema is a good idea for performance if you know what it is. Usually you can infer schema during development and use it as a helper to build the schema to use for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzaLUc5frL2X"
   },
   "outputs": [],
   "source": [
    "#  |-- productid: short (nullable = true)\n",
    "#  |-- productname: string (nullable = true)\n",
    "#  |-- supplierid: short (nullable = true)\n",
    "#  |-- categoryid: short (nullable = true)\n",
    "#  |-- quantityperunit: string (nullable = true)\n",
    "#  |-- unitprice: decimal(10,0) (nullable = true)\n",
    "#  |-- unitsinstock: short (nullable = true)\n",
    "#  |-- unitsonorder: short (nullable = true)\n",
    "#  |-- reorderlevel: short (nullable = true)\n",
    "#  |-- discontinued: integer (nullable = true)\n",
    "\n",
    "prodSchema = StructType([\n",
    "    StructField('ProductID', IntegerType()), \n",
    "    StructField('ProductName', StringType()),\n",
    "    StructField('SupplierID', IntegerType()), \n",
    "    StructField('CategoryID', IntegerType()), \n",
    "    StructField('QuantityPerUnit', StringType()), \n",
    "    StructField('UnitPrice', FloatType()), \n",
    "    StructField('UnitsInStock', IntegerType()), \n",
    "    StructField('UnitsOnOrder', IntegerType()), \n",
    "    StructField('ReorderLevel', IntegerType()), \n",
    "    StructField('Discontinued', IntegerType())\n",
    "])\n",
    "\n",
    "prod2 = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/products', header=True, schema=prodSchema)\n",
    "#prod2 = spark.read.json('file:///class/datasets/northwind/JSON/products', schema=prodSchema)\n",
    "print(prod2)\n",
    "display(prod2)\n",
    "\n",
    "# prod2 = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/products', header=True, inferSchema=False)\n",
    "# print(prod2)\n",
    "# prodSchema2 = \"ProductID:int, ProductName:string, SupplierID:int, CategoryID:int, QuantityPerUnit:string, UnitPrice:double, UnitsInStock:int, UnitsOnOrder:int, ReorderLevel:int, Discontinued:int\"\n",
    "\n",
    "# prod3 = prod2.toDF(prodSchema2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqpW0HPKJxv2"
   },
   "source": [
    "### Convert a DataFrame into a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I64RwvUdJxv3"
   },
   "outputs": [],
   "source": [
    "print (cat1.toJSON().take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or you can chain the whole thing together into a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/categories', inferSchema=True, header=True).toJSON().collect()\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn23EMQcrL2h"
   },
   "source": [
    "### JSON is another top level supported format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypuDVM-crL2i"
   },
   "outputs": [],
   "source": [
    "cat6 = spark.read.json('file:///class/datasets/northwind/JSON/categories')\n",
    "display(cat6)\n",
    "print(cat6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upJodEzYrL2p"
   },
   "source": [
    "### You can also use schemas but be careful of case because it matches the schema names to the key names in the JSON and upper/lower case matters. This will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUo_rYPIrL2t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prod = spark.read.json('file:///class/datasets/northwind/JSON/products', schema=prodSchema)\n",
    "display(prod)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine the schema using all lower case since that's what this particular JSON file has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodSchema = StructType([\n",
    "    StructField('productid', IntegerType()), \n",
    "    StructField('productname', StringType()),\n",
    "    StructField('supplierid', IntegerType()), \n",
    "    StructField('categoryid', IntegerType()), \n",
    "    StructField('quantityperunit', StringType()), \n",
    "    StructField('unitprice', FloatType()), \n",
    "    StructField('unitsinstock', IntegerType()), \n",
    "    StructField('unitsonorder', IntegerType()), \n",
    "    StructField('reorderlevel', IntegerType()), \n",
    "#    StructField('joey', StringType()),\n",
    "    StructField('discontinued', IntegerType())\n",
    "])\n",
    "\n",
    "prod = spark.read.json('file:///class/datasets/northwind/JSON/products', schema=prodSchema)\n",
    "print(prod)\n",
    "display(prod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJAzAmRjrL2z"
   },
   "source": [
    "### You may also see the older style syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4Lh5zjLrL20"
   },
   "outputs": [],
   "source": [
    "prod = spark.read.format('json').load('file:///class/datasets/northwind/JSON/products')\n",
    "display(prod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bky6-O0IrL23"
   },
   "source": [
    "### Writing a DataFrame uses a similar syntax. There is a safeguard against accidentally overwriting a destination, so that's why we are deleting it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "laWiPGfQrL24"
   },
   "outputs": [],
   "source": [
    "! rm -r /tmp/prodjson\n",
    "prod.write.json('file:///tmp/prodjson')\n",
    "#! cat /tmp/prodjson/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively generate a unique filename with a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtB0MwNcrL27"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prod.write.csv(f'file:///tmp/prodcsv{time.strftime(\"%Y%m%d-%H%M%S\")}', sep = '|', header=True)\n",
    "! cat /tmp/prodcsv*/*\n",
    "! ls /tmp/prodcsv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFGU69hUrL3B"
   },
   "source": [
    "### Note the use of mode('overwrite') here as an alternative to deleting it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ek3P9sg0rL3C"
   },
   "outputs": [],
   "source": [
    "prod.write.mode('overwrite').orc('file:///tmp/prodorc')\n",
    "! cat /tmp/prodorc/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHHR9zE1rL3G"
   },
   "outputs": [],
   "source": [
    "prod.write.mode('overwrite').parquet('file:///tmp/prodparquet')\n",
    "! cat /tmp/prodparquet/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Diq6HSPzrL3J"
   },
   "source": [
    "### AVRO is a little different, it is built in now but doesn't have a top level method for it, so you need to use the old style syntax.\n",
    "This doesn't always work inside of a notebook either, so take a look at the program and run it from spark-submit with the proper package dependency added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "954CEsFMrL3K"
   },
   "outputs": [],
   "source": [
    "! cat /class/avro.py\n",
    "\n",
    "! spark-submit --packages org.apache.spark:spark-avro_2.11:2.4.3 /class/avro.py\n",
    "        \n",
    "#prod4.write.format(\"avro\").mode('overwrite').save('/tmp/prodavro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDrJv-aZrL3N"
   },
   "source": [
    "## LAB: ## \n",
    "### Try to read in a few files with different formats and write them out to other formats. \n",
    "### Read shippers found in TSV and write it out as JSON.\n",
    "### Read orders found in CSVHeaders and write it out as ORC.\n",
    "### Read orderdetails found in JSON and write it out as Parquet.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use the syntax that is easiest for you\n",
    "<br>\n",
    "If there is a top level function for the file format, that is usually the easiest way\n",
    "<br>\n",
    "TSV is just the same as CSV, but if there are no headers you need to supply a schema\n",
    "<br>\n",
    "Remember to either remove the destination folder before writing or use an overwrite option\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "shipperSchema = StructType([\n",
    "    StructField('shipperid', StringType()), \n",
    "    StructField('shippername', StringType()),\n",
    "    StructField('phone', StringType())\n",
    "])\n",
    "\n",
    "shippers = spark.read.csv('file:///class/datasets/northwind/TSV/shippers', sep='\\t', header=False, inferSchema=False, schema=shipperSchema)\n",
    "shippers.write.mode('overwrite').json('file:///tmp/shippersjson')\n",
    "\n",
    "orders = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/orders', header=True, inferSchema=True)\n",
    "orders.write.mode('overwrite').orc('file:///tmp/ordersorc')\n",
    "\n",
    "orderdetails = spark.read.json('file:///class/datasets/northwind/JSON/orderdetails')\n",
    "orderdetails.write.mode('overwrite').parquet('file:///tmp/orderdetailsparquet')\n",
    "\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTe379JLrL3O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7TI07ZPrL3X"
   },
   "source": [
    "### Can start to call methods on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0JDyefiJxv7"
   },
   "outputs": [],
   "source": [
    "prod.printSchema()\n",
    "print (prod.columns, prod.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2PVAJGSJxv-"
   },
   "source": [
    "### Choose particular columns from a DataFrame.\n",
    "You can use quoted strings for the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shp65QEQJxv_"
   },
   "outputs": [],
   "source": [
    "display(prod.select('productid', 'productname', 'unitprice'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TuZyScErL3i"
   },
   "source": [
    "### Or you can use a pythonic syntax using the DataFrame name and field name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U96JctQ0rL3i"
   },
   "outputs": [],
   "source": [
    "display(prod.select(prod.productid, prod.productname, prod.unitprice, 'categoryid'))\n",
    "\n",
    "fields = ['categoryid', 'productid']\n",
    "display(prod.select(fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1D_4iv3WrL3l"
   },
   "source": [
    "### Case is ignored if you use quoted strings but not if you use python syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFILXweIrL3m"
   },
   "outputs": [],
   "source": [
    "display(prod.select('Productid', 'productname', 'unitprice'))\n",
    "\n",
    "# this will fail\n",
    "#display(prod.select(prod.Productid, prod.productname, prod.unitprice))\n",
    "\n",
    "# this will not fail\n",
    "display(prod.select(prod.productid, prod.productname, prod.unitprice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTcz7pkirL3p"
   },
   "source": [
    "### `distinct` is a method after the `select` method chooses the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOXLR2IPJxwD"
   },
   "outputs": [],
   "source": [
    "display(prod.select('CategoryID').distinct())\n",
    "\n",
    "# SELECT DISTINCT categoryid from prod\n",
    "# SELECT categoryID from prod GROUP BY categoryid\n",
    "# FROM prod SELECT categoryID DISTINCT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9NySqdqJxwH"
   },
   "source": [
    "### Sort a DataFrame. The `sort` and `orderBy` methods are different aliases for the exact same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9IkEP-4JxwI"
   },
   "outputs": [],
   "source": [
    "field = 'productid'\n",
    "# display(prod.sort(prod.unitprice))\n",
    "# display(prod.orderBy('unitprice', ascending = False))\n",
    "display(prod.select(field, 'productname', prod.unitprice).orderBy('unitprice'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6zM9-5MJxwS"
   },
   "source": [
    "### Create a new DataFrame with a new calculated column added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfXZYr82JxwU"
   },
   "outputs": [],
   "source": [
    "prod2 = (prod.withColumn('value', prod.unitprice * prod.unitsinstock)\n",
    "             .withColumn('other', prod.unitprice * 10))\n",
    "display(prod2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xImpvfmVJxwb"
   },
   "source": [
    "### Remove an unwanted column from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prod.columns)\n",
    "prod.select('categoryid',\n",
    " 'discontinued',\n",
    " 'productid',\n",
    " 'productname',\n",
    " 'reorderlevel',\n",
    " 'supplierid',\n",
    " 'unitprice',\n",
    " 'unitsinstock',\n",
    " 'unitsonorder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `drop` is a convenient way to remove a column without enumerating through all the columns you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvbveAjfJxwd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prod2 = prod2.drop('quantityperunit')\n",
    "display(prod2)\n",
    "# SELECT * EXCEPT(col1, col2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsHhY9u9Jxwh"
   },
   "source": [
    "### The `filter` and `where` methods can both be used and have alternative ways to represent the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bg7uhoEJxwj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = prod\n",
    "display(p.filter(p.unitprice > 100))\n",
    "display(p.filter('unitprice > 100'))\n",
    "\n",
    "# Note == when using python syntax\n",
    "display(p.where(p.categoryid == 2))\n",
    "\n",
    "# Note = when using quoted SQL like syntax\n",
    "display(p.where('categoryid = 2'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUnUFFZXrL3-"
   },
   "source": [
    "### More complex conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_MDNTlbrL4A",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(p.where('unitprice >= 50 and unitprice <= 100'))\n",
    "display(p.where('unitprice between 50 and 100'))\n",
    "\n",
    "display(p.where((p.unitprice >=50) and (p.unitprice <= 100)))\n",
    "\n",
    "# fails\n",
    "# display(p.where((p.unitprice >=50) and (p.unitprice <= 100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5E4afFTJxwp"
   },
   "source": [
    "## LAB: ## \n",
    "### Find all the products in category 2 with fewer units in stock than units on order \n",
    "### Only display with productid, name, unitsinstock, unitsonorder and unitprice\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use the where or filter method. It's probably easier to use a quoted SQL style syntax\n",
    "<br>\n",
    "Use select to get the columns you want to see\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "display(p.where('unitsinstock < unitsonorder and categoryid = 2')\n",
    "         .select('productid','productname', 'unitsinstock', 'unitsonorder', 'unitprice'))\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kZwJv9LJxwr"
   },
   "outputs": [],
   "source": [
    "display(p.where('unitsinstock < unitsonorder and categoryid = 2')\n",
    "         .select('productid','productname', 'unitsinstock', 'unitsonorder', 'unitprice'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRYHW5DlJxwx"
   },
   "source": [
    "### JOINs work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLGYJT1PJxwy"
   },
   "outputs": [],
   "source": [
    "tab1 = sc.parallelize([(1, 'Alpha'), (2, 'Beta'), (3, 'Delta')]).toDF('ID:int, code:string')\n",
    "tab2 = sc.parallelize([(100, 'One', 1), (101, 'Two', 2), (102, 'Three', 1), (103, 'Four', 4)]).toDF('ID:int, name:string, parentID:int')\n",
    "# tab1.join(tab2, tab1.ID == tab2.parentID).show()\n",
    "# tab1.join(tab2, tab1.ID == tab2.parentID, 'left').show()\n",
    "# tab1.join(tab2, tab1.ID == tab2.parentID, 'right').show()\n",
    "# tab1.join(tab2, tab1.ID == tab2.parentID, 'full').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbKSw8mFJxw1"
   },
   "source": [
    "###  Examples of aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Q-LPpNaJxw2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab3 = sc.parallelize([(1, 10), (1, 20), (1, 30), (2, 40), (2,50)]).toDF('groupID:int, amount:int')\n",
    "tab3.groupby('groupID').max().show()\n",
    "#tab3.groupby('groupID').sum().drop('sum(groupID)').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively there is a `agg` method that takes a `dict` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iArjkg9ErL4b"
   },
   "outputs": [],
   "source": [
    "x = tab3.groupby('groupID')\n",
    "x.agg({'amount':'sum'}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or we could import the functions from pyspark and use them directly. This allows us to call multiple aggregates at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsi0GmVyrL4e"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "#from pyspark.sql.functions import sum, max\n",
    "x.agg(F.sum('amount'), F.max('amount')).show()\n",
    "#dir(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or we could use SQL syntax to encode the calculations but we need to use the `expr` function to tell it how to interpret the SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VB_yhwpjrL4j"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "f = expr('sum(amount) as total')\n",
    "print(f, type(f))\n",
    "x.agg(expr('sum(amount) as total'), expr('count(*) as cnt')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDGFU2bwJxxN"
   },
   "source": [
    "## LAB: ## \n",
    "### Join products and categories together displaying only the product and category ID's and names, sort by categoryid and productid\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Make sure not to show the common column twice\n",
    "<br>\n",
    "Select with python style makes it easier to distinguish which columns you want from a join\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "c = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/categories'\n",
    "                   , inferSchema = True, header=True)\n",
    "p = spark.read.json('file:///class/datasets/northwind/JSON/products')\n",
    "\n",
    "# display(c)\n",
    "j = (c.join(p, c.CategoryID == p.categoryid)\n",
    "      .select(c.CategoryID, c.CategoryName, p.productid, p.productname)\n",
    "      .orderBy('categoryid', 'productid'))\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTxpSZd7JxxP",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can also convert a DataFram back to and RDD if we want to use low level RDD methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j1 = j.rdd\n",
    "print(j.rdd.map(lambda x : (x.CategoryID, x.productid)).toDF(['x','y']).take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pxxBU4AJxxS"
   },
   "source": [
    "### Sometimes you want to just rename a column so here are two ways to accomplish that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RUqQyieJxxU"
   },
   "outputs": [],
   "source": [
    "display(p.withColumnRenamed('unitprice','listprice'))\n",
    "\n",
    "cols = p.columns # get a list of all the current column names\n",
    "cols[5] = 'listprice' # replace a column position with the new name \n",
    "p1 = p.toDF(*cols) # create a new dataframe from the original with a list of column names\n",
    "display(p1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GudZR4A_Jxxs"
   },
   "source": [
    "## HOMEWORK: ## \n",
    "### Join Orders, OrderDetails and Products together. Find the sales total for each category listed in descending order by sales\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Load each file into a dataframe and give them single letter aliases for simplicity\n",
    "<br>\n",
    "Join products and order details together on productid\n",
    "<br>\n",
    "Join that to orders on orderid\n",
    "<br>\n",
    "Createa a calculated column to get the line total for each order details\n",
    "<br>\n",
    "Group by categoryID and calculate the sum of the line totals \n",
    "<br>\n",
    "Sort on the calculated total\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "prodSchema = StructType([\n",
    "    StructField('productid', IntegerType()), \n",
    "    StructField('productname', StringType()),\n",
    "    StructField('supplierid', IntegerType()), \n",
    "    StructField('categoryid', IntegerType()), \n",
    "    StructField('quantityperunit', StringType()), \n",
    "    StructField('unitprice', FloatType()), \n",
    "    StructField('unitsinstock', IntegerType()), \n",
    "    StructField('unitsonorder', IntegerType()), \n",
    "    StructField('reorderlevel', IntegerType()), \n",
    "    StructField('discontinued', IntegerType())\n",
    "])\n",
    "\n",
    "\n",
    "```\n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRz6DsSUrL4w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day2-DataFrames.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
